{"cells":[{"cell_type":"markdown","source":["# On colab, switch to GPU (Edit>>Notebook settings>>Hardware Accelerator), not CPU. Is >12x faster (eg1. for align_cca, GPU 20s, CPU >240s and it still hadnt finished running; eg2. for step 5A train_model.py, GPU 104s faster by 2x, CPU 191s)"],"metadata":{"id":"BVEqGQEurvNy"}},{"cell_type":"markdown","source":["# To wrap cell outputs"],"metadata":{"id":"OXQ3s_6XzgAv"}},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"jMA4X-5Vzdtp","executionInfo":{"status":"ok","timestamp":1650270843651,"user_tz":-480,"elapsed":523,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["# Import packages"],"metadata":{"id":"uHGRflSEAop4"}},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"cwHlab73dMic","executionInfo":{"status":"ok","timestamp":1650274586997,"user_tz":-480,"elapsed":559,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"216380b6-cb8b-478d-de0d-c40f01b3f894"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pickle\n","from tqdm import tqdm\n","import os\n","\n","from gensim.models.word2vec import Word2Vec\n","import numpy as np\n","from sklearn.cross_decomposition import CCA\n","from sklearn.metrics.pairwise import cosine_similarity\n","import json\n","from nltk.corpus import stopwords, wordnet\n","import nltk\n","nltk.download('wordnet')\n","import random"]},{"cell_type":"code","source":["# install new packages\n","!pip install translators --upgrade;\n","!pip install langdetect;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-pXc_0jhQdN","executionInfo":{"status":"ok","timestamp":1650268430595,"user_tz":-480,"elapsed":8995,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"f6907075-6880-4242-8364-d26e61e1b2f3"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: translators in /usr/local/lib/python3.7/dist-packages (5.0.2)\n","Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from translators) (4.8.0)\n","Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.7/dist-packages (from translators) (2.27.1)\n","Requirement already satisfied: pathos>=0.2.8 in /usr/local/lib/python3.7/dist-packages (from translators) (0.2.8)\n","Requirement already satisfied: loguru>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from translators) (0.6.0)\n","Requirement already satisfied: PyExecJS>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from translators) (1.5.1)\n","Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.0)\n","Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.70.12.2)\n","Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.4)\n","Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (1.6.6.4)\n","Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from ppft>=1.6.6.4->pathos>=0.2.8->translators) (1.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2021.10.8)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.0.12)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"N4b4_zg3dMik"},"source":["# Worldview & Ideology Analysis"]},{"cell_type":"markdown","metadata":{"id":"kUIlosmXdMin"},"source":["This notebook contains examples of how to perform the analysis from \"Aligning Multidimensional Worldviews and Discovering Ideological Differences\" (Milbauer et al., 2021)"]},{"cell_type":"markdown","metadata":{"id":"Wwkv6QTGdMio"},"source":["## If editing in Google Colab: mount gdrive, clone github repo (if not already cloned), and go to cloned folder"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZh9EyxgdMip","executionInfo":{"status":"ok","timestamp":1650268433403,"user_tz":-480,"elapsed":2818,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"59dfae49-2ef9-4c08-c2e6-b32904d088a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount gdrive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5DaogstdMip","executionInfo":{"status":"ok","timestamp":1650268433405,"user_tz":-480,"elapsed":43,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"e84b54b8-4070-4e2d-e41e-28f33f48bd8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Colab_Notebooks/worldview-ideology/'\n","/content/drive/MyDrive/Colab_Notebooks/worldview-ideology\n","Analysis.ipynb\tdata\t    preprocessing.py  src\tutils.py\n","corpus\t\texample.sh  __pycache__       train.py\n"]}],"source":["# %cd drive/MyDrive/Colab_Notebooks #go to main Colab folder \n","# ! git clone 'https://github.com/limyuzheng88/worldview-ideology.git' #clone\n","# ! git pull #if already cloned repo, then just pull\n","\n","%cd drive/MyDrive/Colab_Notebooks/worldview-ideology/\n","!ls"]},{"cell_type":"code","source":["# To commit and push only this ipynb to the repository, from within a Colab notebook: click File → Save a copy in GitHub .\n","\n","# !git init\n","# !git config --global user.email 'limyuzheng88@yahoo.com.sg'\n","# !git config --global user.name 'limyuzheng88'\n","\n","!git add .\n","# !git status\n","!git commit -m '1. updated Analysis.ipynb'\n","# !git remote add origin https://limyuzheng88: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@github.com/limyuzheng88/worldview-ideology.git\n","# # !git remote remove origin  # if need to redo remote add origin\n","!git push -u origin master\n","\n"],"metadata":{"id":"5ShOmKH31qAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2cUZOKRZdMiq"},"source":["## Load corpora"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iYctBuIudMir","executionInfo":{"status":"ok","timestamp":1650268207204,"user_tz":-480,"elapsed":3967,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["source_a, source_b = 'subreddit_askmen', 'subreddit_askwomen'\n","with open('./corpus/raw/{}.txt'.format(source_a), encoding='utf-8') as f:\n","    corpus_a = f.readlines()\n","with open('./corpus/raw/{}.txt'.format(source_b), encoding='utf-8') as f:\n","    corpus_b = f.readlines()"]},{"cell_type":"code","source":["import translators as ts\n","from langdetect import detect\n","\n","print(corpus_a[0])\n","print(ts.google(corpus_a[0], from_language='en', to_language='zh-CN'))\n","print(ts.google('i pay for this river bank', from_language='en', to_language='zh-CN')) #test ambiguous translations\n","detect('狗屎。 总是一个条件 https://google.com')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"cd91wdLVh1-D","executionInfo":{"status":"ok","timestamp":1650268216455,"user_tz":-480,"elapsed":4089,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"fce12cf9-f64c-48b0-e951-26bc2351d648"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Using United States server backend.\n"]},{"output_type":"stream","name":"stdout","text":["shit . always a condition .\n","\n","狗屎。 总是一个条件。\n","我支付这条河岸\n"]},{"output_type":"execute_result","data":{"text/plain":["'zh-cn'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Preprocess text, run scripts"],"metadata":{"id":"_h2EywDPP0Cq"}},{"cell_type":"code","source":["# # 1. process the raw tokenized data into prepared phrases\n","# !python preprocessing.py corpus/raw/ corpus/prep/;\n","\n","# # 2. compute the word frequencies for each worldview file. This will give you the shared vocabularies to use for alignment\n","# !python ./src/stats/counts.py ./corpus ./data/counts.json\n","\n","# a general-purpose embedding to use for word clustering. You need to choose a text file that best represents \"generic\" language among your communities. You could use the union of the worldview files. That's what we did, and the method implemented by multitrain.sh\n","# 3. First, build the general-purpose embedding:\n","DIMS=100\n","WINDOW=5\n","SAMPLE='.00001'\n","!python ./src/modeling/train_model.py ./corpus/raw ./data/master.model $DIMS $WINDOW $SAMPLE \"multi\"\n","\n","# # 4. Then, compute the clusters: \n","!python ./src/stats/topics.py ./data/master.model ./data/clusters.json\n","\n","# 5. Place your preprocessed text file, each representing one worldview, into corpus/. Then\n","# 5A. train all files in ./corpus...\n","# for filename in tqdm(os.listdir(src)):\n","#   if '.txt' in filename: #only train the datasets (which shall be .txt files)\n","#     src_filepath = './corpus/raw/{}'.format(filename)\n","#     target='./data/models/{}.model'.format(''.join(filename.split('.')[:-1]))\n","#     !python ./src/modeling/train_model.py $src_filepath $target $DIMS $WINDOW $SAMPLE\n","\n","# 5B. or train files individually in ./corpus...\n","for source in tqdm([source_a, source_b]): #edit list as desired, to desired filenames (without extension)\n","  src_filepath = './corpus/prep/{}.txt'.format(source)\n","  target = './data/models/{}.model'.format(source)\n","  !python ./train.py $src_filepath $target;\n","\n","# # Run one of the following aligners:\n","# !python ./src/alignment/align.py ./data/models/ ./data/aligners/cca/ ./data/counts.json cca 1000\n","!python ./src/alignment/align.py ./data/models/ ./data/aligners/svd/ ./data/counts.json svd 1000 # (Khudabukhsh, et al.)\n","# # !python ./src/alignment/align.py ./data/models/ ./data/aligners/lstsq/ ./data/counts.json lstsq 1000\n","\n","# # Check out each of the analysis notebooks for ideas."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RLDrSxrPrznp","outputId":"e8f6ae89-3a43-47b9-8aec-920beb116173"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO:gensim.models.word2vec:collecting all words and their counts\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 549272 words, keeping 17616 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 1079859 words, keeping 24703 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 1608392 words, keeping 30241 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 2148660 words, keeping 34641 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 2705188 words, keeping 38614 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 3231817 words, keeping 42279 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 3768673 words, keeping 45575 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 4311631 words, keeping 48613 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 4885894 words, keeping 51572 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 5336944 words, keeping 54347 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 5738988 words, keeping 56992 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 6124550 words, keeping 59352 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 6547066 words, keeping 61751 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 6946265 words, keeping 64025 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 7308504 words, keeping 65950 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 7765741 words, keeping 68167 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 8189609 words, keeping 70220 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 8597661 words, keeping 72035 word types\n","INFO:gensim.models.word2vec:collected 73309 word types from a corpus of 8875215 raw words and 186923 sentences\n","INFO:gensim.models.word2vec:max_final_vocab=15000 and min_count=100 resulted in calc_min_count=11, effective_min_count=100\n","INFO:gensim.models.word2vec:Loading a fresh vocabulary\n","INFO:gensim.models.word2vec:effective_min_count=100 retains 3957 unique words (5% of original 73309, drops 69352)\n","INFO:gensim.models.word2vec:effective_min_count=100 leaves 8398252 word corpus (94% of original 8875215, drops 476963)\n","INFO:gensim.models.word2vec:deleting the raw counts dictionary of 73309 items\n","INFO:gensim.models.word2vec:sample=1e-05 downsamples 2346 most-common words\n","INFO:gensim.models.word2vec:downsampling leaves estimated 1254949 word corpus (14.9% of prior 8398252)\n","INFO:gensim.models.base_any2vec:estimated required memory for 3957 words and 100 dimensions: 5144100 bytes\n","INFO:gensim.models.word2vec:resetting layer weights\n","INFO:gensim.models.base_any2vec:training model with 2 workers on 3957 vocabulary and 100 features, using sg=1 hs=0 sample=1e-05 negative=5 window=5\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 14.43% examples, 200193 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 29.40% examples, 203357 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 43.81% examples, 203764 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 61.07% examples, 205250 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 81.59% examples, 207533 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 99.53% examples, 207388 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 8875215 raw words (1255116 effective words) took 6.0s, 207723 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 14.79% examples, 205683 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 29.59% examples, 205046 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 44.37% examples, 207109 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 62.14% examples, 208397 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 82.44% examples, 209779 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 8875215 raw words (1254912 effective words) took 5.9s, 211279 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 14.90% examples, 206858 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 29.98% examples, 208111 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 44.91% examples, 209563 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 62.90% examples, 209992 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 83.17% examples, 211473 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 8875215 raw words (1253413 effective words) took 5.9s, 212621 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 14.79% examples, 206492 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 29.98% examples, 208402 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 44.68% examples, 208284 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 62.14% examples, 208590 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 82.52% examples, 210135 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 8875215 raw words (1255808 effective words) took 5.9s, 211841 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 14.90% examples, 207374 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 29.76% examples, 207429 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 44.49% examples, 208095 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 61.90% examples, 208148 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 82.15% examples, 209728 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 8875215 raw words (1255577 effective words) took 5.9s, 211054 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 12.39% examples, 173307 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 20.71% examples, 143751 words/s, in_qsize 2, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 31.34% examples, 145312 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 40.03% examples, 138618 words/s, in_qsize 2, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 46.90% examples, 130684 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 63.51% examples, 140433 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 75.02% examples, 138376 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 87.75% examples, 138803 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 99.56% examples, 137792 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 8875215 raw words (1255556 effective words) took 9.1s, 137999 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 9.60% examples, 134097 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 17.60% examples, 122303 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 27.40% examples, 127209 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 35.77% examples, 124577 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 45.56% examples, 127516 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 64.29% examples, 142466 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 84.07% examples, 153033 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 8875215 raw words (1254678 effective words) took 7.8s, 160048 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 13.36% examples, 186988 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 22.79% examples, 158884 words/s, in_qsize 2, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 31.24% examples, 145467 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 43.00% examples, 150025 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 60.15% examples, 162644 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 80.98% examples, 172224 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 8875215 raw words (1255731 effective words) took 7.0s, 178863 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 14.60% examples, 203457 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 29.98% examples, 208142 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 45.01% examples, 209891 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 62.90% examples, 210027 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 83.17% examples, 211642 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 9 : training on 8875215 raw words (1255652 effective words) took 5.9s, 213893 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 15.15% examples, 211317 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 30.16% examples, 208992 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 44.85% examples, 209094 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 61.32% examples, 206331 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 74.33% examples, 193221 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 86.73% examples, 183769 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 10 : training on 8875215 raw words (1254493 effective words) took 7.0s, 179729 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 14.60% examples, 204066 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 29.50% examples, 205628 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 44.37% examples, 207879 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 62.56% examples, 210040 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 82.85% examples, 211407 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 11 : training on 8875215 raw words (1256383 effective words) took 5.9s, 212085 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 14.34% examples, 199071 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 29.50% examples, 205014 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 44.25% examples, 206327 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 62.14% examples, 208259 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 82.52% examples, 209876 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 12 : training on 8875215 raw words (1253668 effective words) took 5.9s, 211283 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 15.15% examples, 211229 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 30.54% examples, 212258 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 45.01% examples, 210332 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 63.19% examples, 211176 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 83.53% examples, 212821 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 13 : training on 8875215 raw words (1253823 effective words) took 5.9s, 212789 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 14.99% examples, 208363 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 30.07% examples, 208991 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 45.10% examples, 210525 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 63.82% examples, 212545 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 83.95% examples, 213910 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 14 : training on 8875215 raw words (1255208 effective words) took 5.8s, 215240 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 14.79% examples, 206428 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 30.07% examples, 209119 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 45.01% examples, 210496 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 63.51% examples, 211946 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 83.53% examples, 212980 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 15 : training on 8875215 raw words (1255470 effective words) took 5.9s, 214607 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 14.90% examples, 207619 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 29.98% examples, 209054 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 44.11% examples, 206123 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 61.62% examples, 207412 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 82.15% examples, 209660 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 16 : training on 8875215 raw words (1254892 effective words) took 6.0s, 210338 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 14.43% examples, 200242 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 29.50% examples, 204906 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 44.01% examples, 205099 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 62.03% examples, 207997 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 82.52% examples, 209962 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 17 : training on 8875215 raw words (1254129 effective words) took 5.9s, 211497 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 14.60% examples, 203645 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 29.68% examples, 206483 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 44.78% examples, 208972 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 63.36% examples, 211585 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 83.73% examples, 213327 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 18 : training on 8875215 raw words (1254841 effective words) took 5.8s, 214639 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 14.79% examples, 206896 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 29.76% examples, 207416 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 44.68% examples, 208758 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 62.56% examples, 209872 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 83.17% examples, 211900 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 19 : training on 8875215 raw words (1254570 effective words) took 5.9s, 213290 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 14.99% examples, 207649 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 30.28% examples, 210094 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 45.01% examples, 210148 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 63.51% examples, 211636 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 83.73% examples, 213189 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 20 : training on 8875215 raw words (1252734 effective words) took 5.8s, 214239 effective words/s\n","INFO:gensim.models.base_any2vec:training on a 177504300 raw words (25096654 effective words) took 125.5s, 199995 effective words/s\n","INFO:gensim.utils:saving Word2Vec object under data/master.model, separately None\n","INFO:gensim.utils:not storing attribute vectors_norm\n","INFO:gensim.utils:not storing attribute cum_table\n","INFO:gensim.utils:saved data/master.model\n","INFO:root:Completed embedding from: raw\n","INFO:gensim.utils:loading Word2Vec object from data/master.model\n","INFO:gensim.utils:loading wv recursively from data/master.model.wv.* with mmap=None\n","INFO:gensim.utils:setting ignored attribute vectors_norm to None\n","INFO:gensim.utils:loading vocabulary recursively from data/master.model.vocabulary.* with mmap=None\n","INFO:gensim.utils:loading trainables recursively from data/master.model.trainables.* with mmap=None\n","INFO:gensim.utils:setting ignored attribute cum_table to None\n","INFO:gensim.utils:loaded data/master.model\n","INFO:root:Built matrix\n","INFO:root:100 clusters.\n","INFO:root:3957 words.\n","INFO:root:Computed clusters\n","INFO:root:Saved.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO:root:Training word2vec for corpus/prep/subreddit_askmen.txt\n","INFO:gensim.models.word2vec:collecting all words and their counts\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 397895 words, keeping 17399 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 796412 words, keeping 24851 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 1170655 words, keeping 30006 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 1602939 words, keeping 34810 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1983451 words, keeping 38753 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 2361034 words, keeping 42007 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 2818754 words, keeping 45461 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 3231275 words, keeping 48310 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 3630730 words, keeping 50939 word types\n","INFO:gensim.models.word2vec:collected 51953 word types from a corpus of 3782446 raw words and 93717 sentences\n","INFO:gensim.models.word2vec:max_final_vocab=10000 and min_count=10 resulted in calc_min_count=11, effective_min_count=11\n","INFO:gensim.models.word2vec:Loading a fresh vocabulary\n","INFO:gensim.models.word2vec:effective_min_count=11 retains 9657 unique words (18% of original 51953, drops 42296)\n","INFO:gensim.models.word2vec:effective_min_count=11 leaves 3682260 word corpus (97% of original 3782446, drops 100186)\n","INFO:gensim.models.word2vec:deleting the raw counts dictionary of 51953 items\n","INFO:gensim.models.word2vec:sample=0.0001 downsamples 419 most-common words\n","INFO:gensim.models.word2vec:downsampling leaves estimated 1435903 word corpus (39.0% of prior 3682260)\n","INFO:gensim.models.base_any2vec:estimated required memory for 9657 words and 300 dimensions: 28005300 bytes\n","INFO:gensim.models.word2vec:resetting layer weights\n","INFO:gensim.models.base_any2vec:training model with 2 workers on 9657 vocabulary and 300 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 10.94% examples, 151124 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 21.61% examples, 151402 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 33.34% examples, 152643 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 43.64% examples, 152675 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 54.91% examples, 152443 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 66.19% examples, 153323 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 75.90% examples, 152878 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 86.41% examples, 152759 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 97.30% examples, 153007 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 3782446 raw words (1436628 effective words) took 9.4s, 153182 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 10.65% examples, 148995 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 21.32% examples, 150106 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 33.06% examples, 151047 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 43.47% examples, 152517 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 55.54% examples, 153518 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 66.70% examples, 153708 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 76.38% examples, 153445 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 87.35% examples, 153105 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 97.76% examples, 152711 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 3782446 raw words (1434791 effective words) took 9.4s, 153049 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 10.94% examples, 152583 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 21.80% examples, 151531 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 33.34% examples, 151563 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 43.27% examples, 151224 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 54.60% examples, 151258 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 65.79% examples, 151545 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 75.10% examples, 151014 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 85.37% examples, 150450 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 96.24% examples, 150878 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 3782446 raw words (1435080 effective words) took 9.5s, 151324 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 10.94% examples, 151872 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 21.61% examples, 151447 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 33.06% examples, 151889 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 42.93% examples, 151142 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 54.25% examples, 150791 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 65.59% examples, 151348 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 74.81% examples, 150737 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 85.63% examples, 150986 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 96.76% examples, 151747 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 3782446 raw words (1436923 effective words) took 9.4s, 152276 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 10.65% examples, 150693 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 21.61% examples, 152008 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 32.80% examples, 151497 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 42.62% examples, 149955 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 53.96% examples, 150905 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 65.38% examples, 151014 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 74.81% examples, 151191 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 85.37% examples, 151403 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 96.24% examples, 151617 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 3782446 raw words (1435576 effective words) took 9.4s, 151935 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 10.65% examples, 149625 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 21.61% examples, 150622 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 33.06% examples, 151748 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 43.27% examples, 151219 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 54.60% examples, 151402 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 66.03% examples, 152074 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 75.34% examples, 151568 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 86.12% examples, 152047 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 96.76% examples, 151942 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 3782446 raw words (1435247 effective words) took 9.5s, 151802 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 9.82% examples, 139129 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 20.77% examples, 145927 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 32.58% examples, 148164 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 42.62% examples, 149219 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 53.96% examples, 150111 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 65.38% examples, 150574 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 74.54% examples, 150084 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 85.12% examples, 150552 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 95.42% examples, 150123 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 3782446 raw words (1436133 effective words) took 9.5s, 150725 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 10.38% examples, 147130 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 21.61% examples, 150944 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 33.34% examples, 151888 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 43.64% examples, 152148 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 55.22% examples, 152890 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 66.19% examples, 152804 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 75.90% examples, 152148 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 86.41% examples, 152445 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 97.30% examples, 152427 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 3782446 raw words (1435895 effective words) took 9.4s, 152926 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 10.65% examples, 149820 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 21.61% examples, 151311 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 33.34% examples, 153084 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 43.83% examples, 153431 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 54.91% examples, 152616 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 66.03% examples, 152304 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 75.63% examples, 152095 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 86.12% examples, 152360 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 97.30% examples, 152386 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 9 : training on 3782446 raw words (1436071 effective words) took 9.4s, 152949 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 10.65% examples, 150256 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 21.61% examples, 151483 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 33.06% examples, 151737 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 43.64% examples, 152915 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 55.54% examples, 153377 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 66.19% examples, 152341 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 75.90% examples, 152166 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 86.12% examples, 151711 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 96.76% examples, 151494 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 10 : training on 3782446 raw words (1435293 effective words) took 9.4s, 152096 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 10.65% examples, 149406 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 21.80% examples, 151745 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 33.63% examples, 153503 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 43.64% examples, 152861 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 54.91% examples, 152426 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 65.79% examples, 151763 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 75.10% examples, 151589 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 85.63% examples, 151441 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 96.47% examples, 151522 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 11 : training on 3782446 raw words (1435823 effective words) took 9.5s, 151837 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 10.38% examples, 145564 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 21.32% examples, 149559 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 33.06% examples, 151385 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 43.27% examples, 151563 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 54.91% examples, 152779 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 66.03% examples, 152810 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 75.34% examples, 151935 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 85.63% examples, 151696 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 97.04% examples, 152629 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 12 : training on 3782446 raw words (1435785 effective words) took 9.4s, 153056 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 10.94% examples, 150588 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 21.80% examples, 152322 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 33.63% examples, 153443 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 43.83% examples, 153781 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 55.22% examples, 153818 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 65.79% examples, 152223 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 75.10% examples, 151848 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 85.63% examples, 151836 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 96.47% examples, 151959 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 13 : training on 3782446 raw words (1435556 effective words) took 9.4s, 152607 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 10.65% examples, 151096 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 21.61% examples, 151542 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 33.06% examples, 152131 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 43.27% examples, 152332 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 54.91% examples, 153166 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 66.44% examples, 153482 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 76.38% examples, 153787 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 87.35% examples, 154186 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 98.27% examples, 154579 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 14 : training on 3782446 raw words (1435596 effective words) took 9.3s, 154968 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 10.94% examples, 153838 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 21.80% examples, 154043 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 33.63% examples, 153391 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 43.83% examples, 152648 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 55.22% examples, 152987 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 66.70% examples, 153530 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 76.61% examples, 153781 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 87.35% examples, 153196 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 98.27% examples, 153675 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 15 : training on 3782446 raw words (1435186 effective words) took 9.3s, 154189 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 10.94% examples, 150793 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 22.06% examples, 154718 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 33.84% examples, 155374 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 44.09% examples, 155142 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 55.54% examples, 154406 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 66.70% examples, 154719 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 76.80% examples, 155037 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 87.85% examples, 155070 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 98.49% examples, 154919 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 16 : training on 3782446 raw words (1435123 effective words) took 9.3s, 155134 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 11.27% examples, 155669 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 22.06% examples, 155682 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 34.10% examples, 157229 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 44.84% examples, 157028 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 56.83% examples, 157433 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 67.34% examples, 156748 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 77.25% examples, 156848 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 88.47% examples, 157166 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 99.71% examples, 157384 words/s, in_qsize 1, out_qsize 1\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 17 : training on 3782446 raw words (1435930 effective words) took 9.1s, 157660 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 10.65% examples, 150618 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 21.61% examples, 153176 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 33.34% examples, 153901 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 43.90% examples, 154171 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 55.86% examples, 155344 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 67.16% examples, 155546 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 77.04% examples, 155313 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 88.06% examples, 155421 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 99.01% examples, 155426 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 18 : training on 3782446 raw words (1436542 effective words) took 9.2s, 155690 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 10.94% examples, 152957 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 22.06% examples, 155839 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 33.84% examples, 155436 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 44.36% examples, 155113 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 55.86% examples, 155048 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 66.91% examples, 155005 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 76.80% examples, 154973 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 87.85% examples, 154917 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 98.49% examples, 154499 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 19 : training on 3782446 raw words (1435983 effective words) took 9.3s, 154984 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 10.94% examples, 152602 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 21.80% examples, 152534 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 33.06% examples, 151046 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 43.27% examples, 151493 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 54.91% examples, 152406 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 66.19% examples, 153102 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 75.90% examples, 153238 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 86.12% examples, 152882 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 97.30% examples, 153178 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 20 : training on 3782446 raw words (1435976 effective words) took 9.4s, 153162 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 10.65% examples, 151264 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 21.61% examples, 153303 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 33.34% examples, 154513 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 43.27% examples, 153313 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 54.60% examples, 153531 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 66.03% examples, 153764 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 75.34% examples, 153234 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 86.12% examples, 153322 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 21 - PROGRESS: at 97.30% examples, 153460 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 21 : training on 3782446 raw words (1437639 effective words) took 9.3s, 154004 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 10.65% examples, 148995 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 21.61% examples, 152211 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 33.63% examples, 154626 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 44.09% examples, 155301 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 55.86% examples, 155773 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 66.91% examples, 155770 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 76.80% examples, 155347 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 87.85% examples, 155331 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 22 - PROGRESS: at 98.49% examples, 155280 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 22 : training on 3782446 raw words (1436038 effective words) took 9.2s, 155695 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 10.98% examples, 148162 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 22.32% examples, 154341 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 33.84% examples, 153637 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 44.64% examples, 154026 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 56.83% examples, 155246 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 67.34% examples, 154201 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 76.80% examples, 153569 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 87.85% examples, 153736 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 23 - PROGRESS: at 99.01% examples, 154170 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 23 : training on 3782446 raw words (1435949 effective words) took 9.3s, 154554 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 11.27% examples, 153173 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 22.60% examples, 156825 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 34.10% examples, 155778 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 44.36% examples, 154234 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 56.19% examples, 154900 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 67.16% examples, 154875 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 76.38% examples, 153611 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 87.35% examples, 153706 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 24 - PROGRESS: at 98.27% examples, 153803 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 24 : training on 3782446 raw words (1435072 effective words) took 9.3s, 154320 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 10.94% examples, 152897 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 21.80% examples, 153722 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 33.63% examples, 153765 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 44.36% examples, 154709 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 56.55% examples, 156031 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 67.34% examples, 156137 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 77.01% examples, 155385 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 87.85% examples, 155279 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 25 - PROGRESS: at 98.49% examples, 154827 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 25 : training on 3782446 raw words (1435869 effective words) took 9.2s, 155272 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 10.38% examples, 146138 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 21.32% examples, 149750 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 33.06% examples, 151972 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 43.27% examples, 152634 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 53.96% examples, 151398 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 65.59% examples, 151413 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 75.34% examples, 152105 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 86.12% examples, 152077 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 26 - PROGRESS: at 97.04% examples, 152318 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 26 : training on 3782446 raw words (1434776 effective words) took 9.4s, 152124 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 10.65% examples, 150169 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 21.80% examples, 152983 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 33.34% examples, 152670 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 43.27% examples, 152342 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 54.91% examples, 152618 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 66.19% examples, 152827 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 75.90% examples, 152825 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 86.70% examples, 152763 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 27 - PROGRESS: at 97.76% examples, 153326 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 27 : training on 3782446 raw words (1435848 effective words) took 9.3s, 153855 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 28 - PROGRESS: at 10.94% examples, 150920 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 28 - PROGRESS: at 22.06% examples, 152742 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 28 - PROGRESS: at 33.84% examples, 153783 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 28 - PROGRESS: at 44.36% examples, 154316 words/s, in_qsize 3, out_qsize 0\n"]}]},{"cell_type":"markdown","metadata":{"id":"I5G6j3oJdMis"},"source":["## Loading the trained embeddings"]},{"cell_type":"markdown","metadata":{"id":"lHHb5O_SdMit"},"source":["First, we load the trained embeddings, and quickly examine them to see if they make sense.\n","We are using small text samples (500k tokens), so embeddings may not be very good."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq2SihsrdMiu","outputId":"084689ac-32df-46ff-fa33-091131a4ab9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["('republican', 0.6307386159896851)\n","('dem', 0.6165011525154114)\n","('democrats', 0.6043610572814941)\n","('democratic', 0.5978788137435913)\n","('dems', 0.49818018078804016)\n","('liberal', 0.48974037170410156)\n","('candidate', 0.4867892265319824)\n","('party', 0.4780939519405365)\n","('republicans', 0.477538526058197)\n","('progressive', 0.4756127595901489)\n","\n","('dem', 0.6980773210525513)\n","('republican', 0.6605644226074219)\n","('democratic', 0.6189409494400024)\n","('democrats', 0.6140726804733276)\n","('party', 0.6009922623634338)\n","('dems', 0.48170965909957886)\n","('liberal', 0.4369395971298218)\n","('leftist', 0.42886313796043396)\n","('left', 0.4251934587955475)\n","('candidates', 0.4231654405593872)\n"]}],"source":["model_a = Word2Vec.load('models/politics.word2vec.model')\n","model_b = Word2Vec.load('models/the_donald.word2vec.model')\n","# pretrained on more data\n","# model_a = Word2Vec.load('models/politics.big.model')\n","# model_b = Word2Vec.load('models/the_donald.big.model')\n","\n","posWords = ['democrat']\n","negWords = []\n","for x in model_a.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)\n","print()\n","for x in model_b.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pE1x-B6dMiv","executionInfo":{"status":"ok","timestamp":1650270676583,"user_tz":-480,"elapsed":1128,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"2957a38a-7b0d-4915-9495-bd9675830cd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["('orgasm', 0.7038615942001343)\n","('sexual', 0.6980441212654114)\n","('horny', 0.6530160903930664)\n","('pleasure', 0.6499814987182617)\n","('intimacy', 0.6482909321784973)\n","('consent', 0.6474172472953796)\n","('oral', 0.6240077018737793)\n","('sexuality', 0.6177167296409607)\n","('wanting', 0.6117582321166992)\n","('porn', 0.6108335852622986)\n","\n","('control', 0.6555414199829102)\n","('birth', 0.6115919351577759)\n","('feminism', 0.611216127872467)\n","('pregnancy', 0.5445745587348938)\n","('birthcontrol', 0.5277174711227417)\n","('parenting', 0.5203535556793213)\n","('gifts', 0.5160411596298218)\n","('pleasure', 0.511846125125885)\n","('hair', 0.5033841133117676)\n","('malefashionadvice', 0.5019433498382568)\n"]}],"source":["# MY DATASET\n","\n","model_a = Word2Vec.load('data/models/{}.model'.format(source_a)) #or use the .word2vec.model (larger, cos trained on more epochs)\n","model_b = Word2Vec.load('data/models/{}.model'.format(source_b))\n","\n","posWords = ['men', 'sex']\n","negWords = ['women']\n","for x in model_a.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)\n","print()\n","for x in model_b.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)"]},{"cell_type":"markdown","metadata":{"id":"e39j4aELdMiw"},"source":["## Aligning the embeddings"]},{"cell_type":"markdown","metadata":{"id":"4jHjk6l0dMiw"},"source":["First, we find the overlapping vocabulary of the two models, and use this to construct an embedding matrix for each model."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"kb81tKDjdMix","executionInfo":{"status":"ok","timestamp":1650270686182,"user_tz":-480,"elapsed":421,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["vocab_a = list(set(model_a.wv.vocab.keys()))\n","vocab_b = list(set(model_b.wv.vocab.keys()))\n","\n","shared_vocab = set.intersection(set(vocab_a),\n","                                set(vocab_b))\n","shared_vocab = list(sorted(list(shared_vocab)))\n","combo_vocab = set.union(set(vocab_a),\n","                                set(vocab_b))\n","\n","w2idx = { w:i for i,w in enumerate(shared_vocab) }\n","a2idx = { w:i for i,w in enumerate(vocab_a) }\n","idx2b = { i:w for i,w in enumerate(vocab_b) }\n","\n","mtxA = np.vstack([model_a.wv[w] for w in shared_vocab])\n","mtxB = np.vstack([model_b.wv[w] for w in shared_vocab])\n","mtxA_ = np.vstack([model_a.wv[w] for w in vocab_a])\n","mtxB_ = np.vstack([model_b.wv[w] for w in vocab_b])"]},{"cell_type":"markdown","metadata":{"id":"-YaxZlcEdMiy"},"source":["We then select only the N most common words as anchors to train our alignment. (If you're using the big model, this won't quite work because the vocabularies are different.)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"RLWSEwkqdMiz","executionInfo":{"status":"ok","timestamp":1650270687686,"user_tz":-480,"elapsed":364,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["counts = pickle.load(open('data/counts.pkl', 'rb'))\n","n = 5000\n","topN = [y for x,y in sorted([(counts[w], w) for w in w2idx if w in counts], reverse=True)][:n] #w2idx is from shared_vocab\n","idxs = [w2idx[w] for w in topN]"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"uKNU65F2dMiz","executionInfo":{"status":"ok","timestamp":1650270688237,"user_tz":-480,"elapsed":3,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["anchorA = mtxA[idxs, :]\n","anchorB = mtxB[idxs, :]"]},{"cell_type":"markdown","metadata":{"id":"dZhgHzESdMi0"},"source":["Next, we use two different techniques for aligning the embeddings: SVD and CCA"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"nB7GnQCadMi0","executionInfo":{"status":"ok","timestamp":1650270690932,"user_tz":-480,"elapsed":355,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def align_svd(source, target):\n","    product = np.matmul(source.transpose(), target)\n","    U, s, V = np.linalg.svd(product)\n","    T = np.matmul(U,V)\n","    return T\n","\n","svd = align_svd(anchorA, anchorB)\n","svdA = mtxA_.dot(svd)\n","svdB = mtxB_"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"XjhIeI8PdMi1","executionInfo":{"status":"ok","timestamp":1650268747537,"user_tz":-480,"elapsed":18876,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def align_cca(source, target):\n","    N_dims = source.shape[1]\n","    cca = CCA(n_components=N_dims, max_iter=2000)\n","    cca.fit(source, target)\n","    return cca\n","\n","cca = align_cca(anchorA, anchorB)\n","ccaA, ccaB = cca.transform(mtxA, mtxB)"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"2uqxbALAdMi1","executionInfo":{"status":"ok","timestamp":1650270701790,"user_tz":-480,"elapsed":339,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def build_translator(a, b, a2idx, idx2b):\n","    sims = cosine_similarity(a, b)\n","    most_sims = np.argsort(sims, axis=1)[:, ::-1]\n","    \n","    def translator(w, k=1):\n","        idx = a2idx[w]\n","        idxs = most_sims[idx, :k]\n","        words = [idx2b[i] for i in idxs]\n","        return words, sims[idx, idxs]\n","    \n","    return translator"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"-NV17SLKdMi1","executionInfo":{"status":"ok","timestamp":1650270703578,"user_tz":-480,"elapsed":549,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["translator = build_translator(svdA, svdB, a2idx, idx2b)"]},{"cell_type":"markdown","metadata":{"id":"QNl7CfbzdMi2"},"source":["## Exploring the Alignment"]},{"cell_type":"markdown","metadata":{"id":"XKssZjy4dMi2"},"source":["We now explore three different ways of using the alignmed embeddings to explore the worldview and ideology of the two communities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnG_ASy2dMi3","outputId":"48fddb2d-e9d4-4a9a-f3ff-15d904848d55"},"outputs":[{"data":{"text/plain":["(['democrat', 'republican', 'dem', 'democrats', 'republicans'],\n"," array([0.6164709 , 0.5891098 , 0.5137549 , 0.4719857 , 0.46580008],\n","       dtype=float32))"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["translator('democrat', k=5)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6d5VJd0dMi3","executionInfo":{"status":"ok","timestamp":1650270720641,"user_tz":-480,"elapsed":339,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"73f006f3-307d-4207-b432-8d8e1e09c0dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['men', 'women', 'misogyny', 'themselves', 'trans'],\n"," array([0.7862248, 0.7762395, 0.6666307, 0.6360846, 0.6330702],\n","       dtype=float32))"]},"metadata":{},"execution_count":54}],"source":["# MY DATASET\n","translator('men', k=5)"]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":989},"id":"UywAIrxBdMi4","executionInfo":{"status":"ok","timestamp":1650275205684,"user_tz":-480,"elapsed":942,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"6037d1c5-c3cd-4921-b803-4a1b4568511e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m\u001b[94mmen\u001b[0m in source_a (1 example sentence below):\n","some \u001b[1m\u001b[94mmen\u001b[0m do n't learn about cleaning their dicks properly early on in life . i learned when i was 17 . not sure who i should be mad at more my parents or health class .\n","\n","translates to source_b words:\n","\n","\u001b[1m\u001b[91mmen\u001b[0m, aligment score: 0.7862247824668884\n","what you mean is work ethic . it encompasses diligence , desire to learn , ability to accept criticism , owning up to mistakes . it has little to do with toxic masculinity . your approach of trying to see who “ reeks of toxic masculinity ” is no different than \u001b[1m\u001b[91mmen\u001b[0m dismissing women on the fact that they are women .\n","\n","\u001b[1m\u001b[91mwomen\u001b[0m, aligment score: 0.7762395143508911\n","military vet here . i filed sexual harassment against my ncoic ( the sergeant in charge ) . he was my supervisor 's supervisor . he had been messaging me inappropriate things on social media for months . things about my ass , things about masturbation , things about other \u001b[1m\u001b[91mwomen\u001b[0m in the office . i finally got the courage to report him when another woman stepped up who had been going through the same thing with him . we reported together . he was laterally moved . no demotion , & the reason for his move was kept secret . in his new shop he was still in a supervisory role with \u001b[1m\u001b[91mwomen\u001b[0m as his subordinate . my shop even got him a going away present when he was moved . i felt crushed & like i had wasted my time talking about it . i was shunned by some of the leadership in the shop from that point on . just treated really differently . it sucked . i really enjoyed my time in the military , but thinking about that situation alone makes me happy that i 'm a civilian now .\n","\n","\u001b[1m\u001b[91mmisogyny\u001b[0m, aligment score: 0.6666306853294373\n","yeah i do n't like her either . idk if it 's like internalized \u001b[1m\u001b[91mmisogyny\u001b[0m or cuz i get like “ i 'm the shit cuz i 'm famous ” vibes or that she just does n't seem like a real human cuz i never hear her opinions about real life issues , nor does she express emotions or communicate with people , etc . but idk , i never got the hype nor will i ever understand the hype lol\n","\n","\u001b[1m\u001b[91mthemselves\u001b[0m, aligment score: 0.6360846161842346\n","exactly ! most women , especially those who are young \u001b[1m\u001b[91mthemselves\u001b[0m , prefer their young counterparts . the main reason younger women tolerate older men is because young men are too non-commital . at least if they get an older man , he has money to spend while being a less than optimal partner .\n","\n","\u001b[1m\u001b[91mtrans\u001b[0m, aligment score: 0.633070170879364\n","omg , i have so much to say about this so sorry in advance for the rant . first off , i just got into an argument with my male therapist about this , and it was so frustrating because it was like , a woman would understand what i was saying immediately . basically , i was telling him about how i was sandwiched between boys growing up and wanted to be my brothers . i told him how i resented growing boobs and wore baggy shirts well into high school because i hated being viewed sexually . i then told him that i actually look back so fondly on my journey to discovering what femininity was to me and realized i was only rebelling against the tiny box * * * i * * * put an entire gender into . i realized that other women are just as diverse in interests and attitudes as everyone else . ( it 's embarrassing to admit that i had to realize this , but whatever . it is what it is . ) but he then asked questions and made comments that hinted that he thought i might be \u001b[1m\u001b[91mtrans\u001b[0m ? i was like , no ... i just told you that i love being a woman and am very comfortable here , i just hated what i did n't understand for a minute there . not to say there 's anything wrong with being \u001b[1m\u001b[91mtrans\u001b[0m , but it was a wrong conclusion to what i was saying . to me , femininity is that beautiful feeling of sisterhood . it 's having friendships where they automatically understand things like what i mentioned above and i know that there 's zero hoping that i 'd one day decide to have sex with them . there 's something so pure and validating about female friends and i value those relationships so much . it 's also the subtle power that women have in this world . it 's the x factor that can make a man ( or another woman , depending ) give up government secrets , abandon roles of power , switch sides , start wars . it 's the fact that we 're biologically smaller and weaker on average , but have still been the backbone of society and humanity since its inception and have so much to offer . but this took years to figure out because of everything i was told by people in my life . i got roasted by boyfriends for not wearing makeup and for my love of sweatpants all the time . i once got makeup as a gift from my ex bf 's family for christmas and was so utterly confused because they had known me for four years at that point and i never wore it . i sat there in shock as they put it on me , and the mom and sister were like , \" we 've been wanting to do this for * so * long . \" i remember being so hurt by that . i really let them make me feel so inadequate as a woman . idk , all of this to say that i would have killed to have someone tell me that being feminine is so much more than i thought it was and truly a powerful thing in like ... middle school .\n","\n"]}],"source":["# MY DATASET\n","class color:\n","   PURPLE = '\\033[95m'\n","   CYAN = '\\033[96m'\n","   DARKCYAN = '\\033[36m'\n","   BLUE = '\\033[94m'\n","   GREEN = '\\033[92m'\n","   YELLOW = '\\033[93m'\n","   RED = '\\033[91m'\n","   BOLD = '\\033[1m'\n","   UNDERLINE = '\\033[4m'\n","   END = '\\033[0m'\n","\n","# function to get example sentences that contain said word  \n","def get_example_sentences(word, corpus, n_sentences=1, seed=None):\n","    random.seed(a=seed) #fix seed if needed. removes randomness, and fixes the output as a constant\n","    sentences = random.sample([s for s in corpus if ' {} '.format(word) in s], n_sentences) #already tokenized when preprocessing data. We search only for tokens with spacing to left & right, eg. ' men ', else 'omen' also gets deemed to contain 'men'. We're only looking for strictly 'men'\n","    return sentences\n","    \n","n_sentences = 1\n","seed = None\n","\n","word_a = 'men'\n","word_a_highlighted = '{}{}{}'.format(color.BOLD + color.BLUE, word_a, color.END)\n","print('{} in source_a ({} example sentence below):'.format(word_a_highlighted, n_sentences))\n","[print(s.replace(' {} '.format(word_a), ' {} '.format(word_a_highlighted))) for s in get_example_sentences(word_a, corpus_a, n_sentences=n_sentences, seed=seed)]\n","\n","word_b = translator(word_a, k=5)\n","print('translates to source_b words:\\n')\n","for w in zip(translator(word_a, k=5)[0], translator(word_a, k=5)[1]):\n","    word_b_highlighted = '{}{}{}'.format(color.BOLD + color.RED, w[0], color.END)\n","    print(word_b_highlighted + ', aligment score: {}'.format(w[1]))\n","    [print(s.replace(' {} '.format(w[0]), ' {} '.format(word_b_highlighted))) for s in get_example_sentences(w[0], corpus_b, n_sentences=n_sentences, seed=seed)]"]},{"cell_type":"markdown","metadata":{"id":"72IoJRuydMi5"},"source":["### Misalignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okYnJ571dMi5","outputId":"8f65a005-8e6a-4e9c-d1b0-2ecbdcd74e50"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3664901664145234\n"]}],"source":["misaligned = []\n","scores = []\n","\n","for w in shared_vocab:\n","    w_ = translator(w)[0][0]\n","    s = translator(w)[1][0]\n","    if w != w_:\n","        misaligned.append((w, w_))\n","        scores.append(s)\n","        \n","print(len(misaligned) / len(shared_vocab))"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"ZND1xzAJdMi5","executionInfo":{"status":"ok","timestamp":1650270921836,"user_tz":-480,"elapsed":543,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"96228d3a-beea-4cb5-b9b9-c7e47f2ad2a1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.3889156626506024\n"]}],"source":["# MY DATASET\n","misaligned = []\n","scores = []\n","\n","for w in shared_vocab:\n","    w_ = translator(w)[0][0]\n","    s = translator(w)[1][0]\n","    if w != w_:\n","        misaligned.append((w, w_))\n","        scores.append(s)\n","        \n","print(len(misaligned) / len(shared_vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_nHF7WjdMi6","outputId":"8d96917c-c0dd-4bf0-fa3c-02e5a17c2e6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["('performed_automatically', 'please_contact') 0.8923226\n","('moderators', 'please_contact') 0.8301286\n","('``', \"''\") 0.7827312\n","('&', 'gt') 0.74673975\n","('bot', 'performed_automatically') 0.7402881\n","(';', 'gt') 0.71963507\n","('though', 'but') 0.7046928\n","('citizenship_question', 'census') 0.68586487\n","('amp', ';') 0.68398106\n","('action', 'performed_automatically') 0.6676772\n","('couple', 'few') 0.6567316\n","('disagree', 'agree') 0.64628285\n","('dems', 'democrats') 0.6362802\n","('supreme_court', 'scotus') 0.61996275\n","('republican', 'democrat') 0.6085014\n","('dumb', 'stupid') 0.60647255\n","('26_times', 'lolita_express') 0.6013237\n","('capitalism', 'communism') 0.5988106\n","('jeffrey_epstein', 'epstein') 0.59700453\n","('illegal_immigrants', 'illegals') 0.5922674\n"]}],"source":["for pair, score in sorted(zip(misaligned, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"RLL76NDXdMi6","executionInfo":{"status":"ok","timestamp":1650270928091,"user_tz":-480,"elapsed":329,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"486029ca-4ee3-4c79-8f1e-c7c96a6c70cb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["('subreddit', 'compose') 0.9350627\n","('performed', 'compose') 0.932218\n","('automatically', 'compose') 0.92873514\n","('concerns', '=/') 0.91821444\n","('bot', 'compose') 0.91762775\n","('moderators', 'compose') 0.9035689\n","('action', 'compose') 0.8891935\n","('shorts', 'pants') 0.8832135\n","('message', 'moderators') 0.88136923\n","('shirts', 'jeans') 0.87320757\n","('[', ']') 0.86907256\n","('r', '=/') 0.8665351\n","('tea', 'coffee') 0.8596305\n","('expensive', 'cheaper') 0.85752565\n","('please', 'message') 0.8567974\n","('shower', 'wash') 0.85311604\n","('tight', 'pants') 0.8521388\n","('9', '5') 0.8500881\n","('19', '18') 0.84579027\n","('paying', 'pay') 0.845045\n"]}],"source":["# MY DATASET\n","for pair, score in sorted(zip(misaligned, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"markdown","metadata":{"id":"4Cyv_kpJdMi7"},"source":["### Antonyms"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"XTds9rLIdMi7","executionInfo":{"status":"ok","timestamp":1650270938953,"user_tz":-480,"elapsed":3244,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"08d87342-5165-4ef8-d42e-e2e192d6daf4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2828/2828 [00:02<00:00, 958.85it/s] \n"]}],"source":["def get_antonyms(vocab):\n","    antonyms = []\n","    for w in tqdm(vocab):\n","        for synset in wordnet.synsets(w):\n","            for lemma in synset.lemmas():\n","                if lemma.antonyms():\n","                    antonyms.append((w, lemma.antonyms()[0].name()))\n","    antonyms = set(antonyms)\n","    return antonyms\n","\n","antonyms = get_antonyms(combo_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYwNg1kddMi7","outputId":"c0adcbeb-3e04-4d2f-9ae2-7e1434f24620"},"outputs":[{"name":"stdout","output_type":"stream","text":["('civilian', 'military')\n","('decrease', 'increase')\n","('disagree', 'agree')\n","('disrespect', 'respect')\n","('illogical', 'logical')\n","('inaccurate', 'accurate')\n","('indirectly', 'directly')\n","('ineffective', 'effective')\n","('intolerant', 'tolerant')\n","('invalid', 'valid')\n","('liability', 'asset')\n","('sell', 'buy')\n","('sells', 'buy')\n","('unreasonable', 'reasonable')\n","('unwilling', 'willing')\n","('weakness', 'strength')\n","('west', 'east')\n"]}],"source":["for mPair in misaligned:\n","    if mPair in antonyms or (mPair[0], mPair[1]) in antonyms:\n","        print(mPair)"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"hFA8NWbOdMi8","executionInfo":{"status":"ok","timestamp":1650270943351,"user_tz":-480,"elapsed":349,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"25b0abb8-f49e-4b4a-a5dd-f9d5c2d68583"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["('children', 'parent')\n","('daughter', 'son')\n","('father', 'mother')\n","('late', 'early')\n","('male', 'female')\n","('more', 'less')\n","('particular', 'general')\n","('second', 'first')\n","('son', 'daughter')\n","('white', 'black')\n","('wife', 'husband')\n"]}],"source":["# MY DATASET\n","for mPair in misaligned:\n","    if mPair in antonyms or (mPair[0], mPair[1]) in antonyms:\n","        print(mPair)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1GGi9UbdMi8","outputId":"e7e01491-e3ca-4e3f-994c-aae31c74c862"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5847953216374269% misaligned pairs from 'misaligned', in 'antonyms' set\n"]}],"source":["print(\"{}% misaligned pairs from 'misaligned', in 'antonyms' set\".format(round(len([mPair for mPair in misaligned if mPair in antonyms or (mPair[0], mPair[1]) in antonyms])/len(misaligned)*100, 2)))"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"C6rDYftsdMi8","executionInfo":{"status":"ok","timestamp":1650271084275,"user_tz":-480,"elapsed":363,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"47844fc0-5d4d-46ac-ef84-a26a40a4ff4e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1.36% misaligned pairs from 'misaligned', in 'antonyms' set\n"]}],"source":["# MY DATASET\n","print(\"{}% misaligned pairs from 'misaligned', in 'antonyms' set\".format(round(len([mPair for mPair in misaligned if mPair in antonyms or (mPair[0], mPair[1]) in antonyms])/len(misaligned)*100, 2)))"]},{"cell_type":"markdown","metadata":{"id":"HckDQAJsdMi9"},"source":["### Translation / Conceptual Homomorphisms"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"JA6GQZf6dMi9","executionInfo":{"status":"ok","timestamp":1650271119273,"user_tz":-480,"elapsed":333,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"5b66b158-d7c1-4187-ca6c-3fd77688016f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["unique_vocab = []\n","for w in model_a.wv.vocab:\n","    if w not in model_b.wv.vocab:\n","        unique_vocab.append(w)"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"GGihCU8GdMi9","executionInfo":{"status":"ok","timestamp":1650271119835,"user_tz":-480,"elapsed":20,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"2416c57c-dafe-4a4b-a940-34ba6aa99666"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["translations = []\n","scores = []\n","for w in unique_vocab:\n","    t = translator(w)\n","    translations.append((w, t[0][0]))\n","    scores.append(t[1][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_tOyQVbdMi-","outputId":"fe6354d3-5572-4d9b-881b-07c939e9088f"},"outputs":[{"name":"stdout","output_type":"stream","text":["('instructions_provided', 'performed_automatically') 0.71877486\n","('permanent_ban', 'performed_automatically') 0.69331694\n","('rule_violations', 'performed_automatically') 0.63353837\n","('wishing_death/physical', 'performed_automatically') 0.594555\n","('fully_participate', 'please_contact') 0.5898004\n","('rulebreaking_content', 'performed_automatically') 0.5775635\n","('`_youtu.be', '`') 0.55210274\n","('spam_domain', 'performed_automatically') 0.5434005\n","('/r/politics_within', 'performed_automatically') 0.52550036\n","('troll_accusations', 'performed_automatically') 0.51061064\n","('whitelisting', 'performed_automatically') 0.4963802\n","('blatant_spam', 'performed_automatically') 0.48971322\n","('confederate_flag', 'flag') 0.48527563\n","('excluding_indians', 'persons') 0.48497242\n","('site_administrators', 'link_shortener') 0.48107997\n","('following_reason', 'submission') 0.48058963\n","('alan_dershowitz', 'epstein') 0.48009375\n","('drinking_water', 'water') 0.47866067\n","('breaking_channel', 'link_shortener') 0.47774062\n","('nonreputable_/', 'performed_automatically') 0.47719014\n"]}],"source":["for pair, score in sorted(zip(translations, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"code","execution_count":121,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lz1sxSJudMi-","executionInfo":{"status":"ok","timestamp":1650275249825,"user_tz":-480,"elapsed":3839,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"58faaebf-88e6-4018-b095-175b8b75a866"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m\u001b[94maskmen\u001b[0m & \u001b[1m\u001b[91m=/\u001b[0m, aligment score: 0.9201257228851318\n","\u001b[1m\u001b[94maskmen\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / \u001b[1m\u001b[94maskmen\u001b[0m ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91m=/\u001b[0m:\n","hello wheredoesallthiscome . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to \u001b[1m\u001b[91m=/\u001b[0m r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mreasoning\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.8699803352355957\n","\u001b[1m\u001b[94mreasoning\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any \u001b[1m\u001b[94mreasoning\u001b[0m , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","your comment has been removed because : your karma is too low to participate on askwomen . you will be able to participate when your karma has increased . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mtowel\u001b[0m & \u001b[1m\u001b[91mwash\u001b[0m, aligment score: 0.8527393937110901\n","\u001b[1m\u001b[94mtowel\u001b[0m:\n","i have a hand \u001b[1m\u001b[94mtowel\u001b[0m that 's sole purpose is for post sex cleanup .\n","\n","\u001b[1m\u001b[91mwash\u001b[0m:\n","sunscreen when the sun is out , \u001b[1m\u001b[91mwash\u001b[0m it with water and a washcloth at least every two days ( everyday if using sunscreen ) , when i start having dry itchy skin gently exfoliate with natural clay and hydrate with a little face oil ( about every two weeks ) .\n","\n","~~~\n","\u001b[1m\u001b[94mboots\u001b[0m & \u001b[1m\u001b[91mpair\u001b[0m, aligment score: 0.8300528526306152\n","\u001b[1m\u001b[94mboots\u001b[0m:\n","a solid pair of brown leather \u001b[1m\u001b[94mboots\u001b[0m . good for all occasions where pants are required . nothing fancy , but real leather , no leather blend .\n","\n","\u001b[1m\u001b[91mpair\u001b[0m:\n","i really like the high waist wide jean styles , they 're not the mom jean style and they 're also not the boyfriend style jeans , like they 're perfectly fit at the waist but loose on the thighs and legs . perfect now during winter when it 's extra cold and you wanna wear a long \u001b[1m\u001b[91mpair\u001b[0m of warm winter leggings under and also so comfy . personally i 'm ready for the mom jean trend to go . i feel like it 's mainly for quirky artsy people . the boyfriend style i can get behind though !\n","\n","~~~\n","\u001b[1m\u001b[94mtaller\u001b[0m & \u001b[1m\u001b[91mheight\u001b[0m, aligment score: 0.8048233389854431\n","\u001b[1m\u001b[94mtaller\u001b[0m:\n","stulp et al ( 2013 ) used a sample of 5782 north american speed-daters making 128,104 choices to determine preferences for partner height and how height influenced the formation of a match . they found that women were most likely to choose a speed-dater 25 cm \u001b[1m\u001b[94mtaller\u001b[0m than themselves , whereas men were most likely to choose women only 7 cm shorter than themselves . as a consequence , matches were most likely at an intermediate height difference ( 19 cm ) that differed significantly from the preferred height difference of both sexes . their data can be further analyzed to provide data about women 's height cutoffs , the benefit of each inch of height for a man , and the degree of competition each man faces based on his height . 1 ) cutoffs : their findings demonstrated the cutoffs at which women consider a man too short or too tall : 90 % of women will reject a man who is 5 ' 4 \" based solely on his height . 65 % of women will reject a man who is 5 ' 7 \" based solely on his height . 50 % of women will reject a man who is 5 ' 8 \" based solely on his height . 14 % of women will reject a man who is 5 ' 10 \" based solely on his height . 1.5 % of women will reject a man who is 6 ' based solely on his height . past 6 ' 2 \" , women begin to increase rejections of men for being too tall . 30 % of women believe there is no such thing as a man being \" too tall . \" over 94 % of women will reject a man solely for him being too short .\n","\n","\u001b[1m\u001b[91mheight\u001b[0m:\n","for me , it 's tough . i 'm an extremely muscular female and i do need a lot of protein , i eat a lot of meat , and i seem to need more calories than other women my same \u001b[1m\u001b[91mheight\u001b[0m and weight . this is just to maintain , i am not trying to gain muscle . i can't go by tdee calculators online because they calculate me as overweight ( fat ) , when in reality it 's muscle . i have done * * a lot * * of trial an error with my diet using [ cronometer.com ] ( HTTPURL ) to determine how high of a protein diet i require . i would suggest this to any woman who is trying to maintain or gain muscle . we are all pretty individual when it comes to diet and what works best for our physiques .\n","\n","~~~\n","\u001b[1m\u001b[94mlifting\u001b[0m & \u001b[1m\u001b[91mexercise\u001b[0m, aligment score: 0.8041852116584778\n","\u001b[1m\u001b[94mlifting\u001b[0m:\n","i 'm 22 , 5 ' 1 , and am thin so my preferences are different from other girls who are taller than me . i do like taller guys . they get treated with more respect which is nice , because i do n't want a guy who gets bullied by his friend group . i prefer guys who are thinner . i like guys with really nice , strong shoulders and arms . idc if he has abs , as long as he 's not overweight . i just like nice arms , shoulder , and back . part of the reason i do n't like big , big guys is because i feel like they make me look like a little kid next to them , which people comment on , and i find creepy . thinner guys are also less likely to go on about how tiny i am which also just makes me feel like a child . one of my friends is like 5 ' 8 and very curvy and she likes big guys because they make her feel smaller and more dainty . they 're also strong enough to lift her which she finds hot then i have another girlfriend who is 5 ' 8 who has a very skinny boyfriend . she 's big into sports ( majorly obsessed with football ) so she was looking for a dude who would n't feel emasculated by her . just like men , we all have different preferences ! but the biggest trend i 've noticed is just a desire to be able to picked up . this does n't mean you should go around \u001b[1m\u001b[94mlifting\u001b[0m random women off the ground , though . last time a guy did that to me , my arms were covered in bruises for a week because he squeezed so hard and my mom became convinced i was hiding an abusive relationship from her\n","\n","\u001b[1m\u001b[91mexercise\u001b[0m:\n","currently , i 'm struggling to learn not to engage in fruitless arguments & walk away from unresolved situations . reddit is helping by giving me plenty of \u001b[1m\u001b[91mexercise\u001b[0m ! :grinning_face_with_sweat:\n","\n","~~~\n","\u001b[1m\u001b[94mpounds\u001b[0m & \u001b[1m\u001b[91mweight\u001b[0m, aligment score: 0.7994028925895691\n","\u001b[1m\u001b[94mpounds\u001b[0m:\n","i have a brussels griffon , which already is pretty rare but he 's 20 \u001b[1m\u001b[94mpounds\u001b[0m and overall much larger than most dogs of his breed\n","\n","\u001b[1m\u001b[91mweight\u001b[0m:\n","eating when i 'm hungry . i hate this idea that you have to starve yourself for no reason in an attempt to lose \u001b[1m\u001b[91mweight\u001b[0m or not be seen as greedy . if i 'm hungry , i 'll eat and i do n't feel bad about it .\n","\n","~~~\n","\u001b[1m\u001b[94mautomated\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7986434102058411\n","\u001b[1m\u001b[94mautomated\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the \u001b[1m\u001b[94mautomated\u001b[0m filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","hello suunniibuunnii . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mflagged\u001b[0m & \u001b[1m\u001b[91mmoderators\u001b[0m, aligment score: 0.7932913899421692\n","\u001b[1m\u001b[94mflagged\u001b[0m:\n","the computer \u001b[1m\u001b[94mflagged\u001b[0m this as a frequently asked question . use the search bar . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mmoderators\u001b[0m:\n","removed for disrespectful commentary . if you have any questions please message the \u001b[1m\u001b[91mmoderators\u001b[0m through the link on the sidebar\n","\n","~~~\n","\u001b[1m\u001b[94msalary\u001b[0m & \u001b[1m\u001b[91mpaid\u001b[0m, aligment score: 0.7878615856170654\n","\u001b[1m\u001b[94msalary\u001b[0m:\n","i 'm much better at spending money than saving it , but other than that i 'm doing pretty well . i have a full-time job working 8 months a year as a counselor in a school . i have a decent \u001b[1m\u001b[94msalary\u001b[0m , husband from whom i 'm separated , two cats , and i just ate hot pockets for dinner . all things considered , i 'm doing a shit-ton better than i should have .\n","\n","\u001b[1m\u001b[91mpaid\u001b[0m:\n","i 'm currently living with my parents bc i got pregnant in march 2020 and it was better this way , but before that i was completely financially independent 3 months after my 18th birthday . i got a crazy good scholarship that covered tuition and gave me a monthly life expense stipend . i used that money to cover rent and basic apartment needs like furniture , cleaning supplies , etc . then i worked . way too much . i was taking 20 credit hours for school , working as a nanny for one family for 15 hours a week , nannying for another family for 12 hours a week , babysitting one-offs and doing odd jobs pretty much every weekend , and got an internship that \u001b[1m\u001b[91mpaid\u001b[0m next to nothing but helped my resume that i worked at for like 8 hours a week . also , i went on a whole lot of tinder dates so i rarely \u001b[1m\u001b[91mpaid\u001b[0m for dinners out , and at home i ate way too much oatmeal , toast , and soup made from powders to keep costs low . must 've done something right , though . i move back to school in august , and it 's looking like i can live off of the money i saved so i do n't have to be away from my baby so much .\n","\n","~~~\n","\u001b[1m\u001b[94mdollars\u001b[0m & \u001b[1m\u001b[91m$\u001b[0m, aligment score: 0.7839125394821167\n","\u001b[1m\u001b[94mdollars\u001b[0m:\n","we definitely like useful lights too . but mostly we 're people who spend several hundred \u001b[1m\u001b[94mdollars\u001b[0m on a new flashlight so that we can go \" neat \" and then pester our clearly uninterested significant other about how cool it is , despite the fact that they obviously did n't care about the last 7 lights we bought .\n","\n","\u001b[1m\u001b[91m$\u001b[0m:\n","it really depends on why you 're angry , bitter , and cynical . in my case it 's because i have a law degree but work a job that pays less than \u001b[1m\u001b[91m$\u001b[0m 15 an hour , have over \u001b[1m\u001b[91m$\u001b[0m 150k in debt , and am renting in a city that 's very expensive so i 'm constantly tired , \" poor \" , and bitter . i 'm working towards passing the bar , and i 'm moving states in july for more opportunities for my current qualifications and education . plus lower rent , thank goodness . i 'm angry , bitter , and cynical because of my current situation .\n","\n","~~~\n","\u001b[1m\u001b[94mweights\u001b[0m & \u001b[1m\u001b[91mexercise\u001b[0m, aligment score: 0.7828139066696167\n","\u001b[1m\u001b[94mweights\u001b[0m:\n","easy . you do n't . that 's like , the golden rule man . it 's right there at the top , followed by putting your \u001b[1m\u001b[94mweights\u001b[0m back and using headphones to listen to music .\n","\n","\u001b[1m\u001b[91mexercise\u001b[0m:\n","i recently have been putting more effort into and taking more time to do skincare , haircare , \u001b[1m\u001b[91mexercise\u001b[0m , and making myself look nice . it has been helping me relax more and it 's just a nice time to reflect and to do some self-care . it 's also been helping me in terms of my confidence since i know that i put effort into my appearance .\n","\n","~~~\n","\u001b[1m\u001b[94mbruh\u001b[0m & \u001b[1m\u001b[91mlol\u001b[0m, aligment score: 0.7804096341133118\n","\u001b[1m\u001b[94mbruh\u001b[0m:\n","shit \u001b[1m\u001b[94mbruh\u001b[0m fkn no one has my first or last name lmaooo\n","\n","\u001b[1m\u001b[91mlol\u001b[0m:\n","it is absolutely worth it . i enjoyed pregnancy , and am sad i will never get to do it again . i felt confident in my body , and continued to exercise which made me feel strong . i loved the kicks , the belly , and i liked having bigger boobs \u001b[1m\u001b[91mlol\u001b[0m . despite my easy pregnancy i had a rough delivery . i went into labor on my own but because she was positioned so high i pushed for three hours and tore . i had been awake 30 + hours at that point and was exhausted . i had some ptsd combined with the physical recovery . that sucked big time . my biggest fear was tearing and despite making such an effort to have a healthy pregnancy i still had a hard time and tore . i did heal normally and do n't have issues now , but at the time i thought i 'd never get better . my daughter is worth it though , no question . i think before getting pregnant , you should drop your expectations . when trying , my doctor told me it could take up to a year . because i tend to be a control freak , i made myself wait until my period was due to take a pregnancy test . i did n't want to obsess and get disappointed . that worked out good for me because it took nine months to get pregnant . when it comes to delivering , do n't have a birth plan . there is a benefit to being educated about the process , but in the end your body is going to do what it 's going to do to get the baby out . you do n't have much control .\n","\n","~~~\n","\u001b[1m\u001b[94msuit\u001b[0m & \u001b[1m\u001b[91mdress\u001b[0m, aligment score: 0.7799713015556335\n","\u001b[1m\u001b[94msuit\u001b[0m:\n","in college i was sort of on top of the world . everything was going right , it shot my confidence levels up , and girls liked that . i initially was seeing one girl . then two . then three . my ego kind of took over . i would finish class and decide which one to hang out with tonight , and just lie to the other two if i was ever asked to do something with them . i kept that rotation going for a couple months , but two of them really liked me and i knew they wanted to become more serious . one night it boiled over . i was at a bar and the one who liked me most ( who was also the sweetest ) saw me with one of the others . huge fight ensues . both girls dip . i realize i 'd become such a piece of shit ... try to get the first one back , she tells me to kick rocks . it was a huge turning point in my life . i 've had two relationships since then and remained super committed to each of them ( i 'm marrying the one i 've got now early next year ! ) . my headspace is so much healthier , my goals in life are much more ethically upstanding , etc . it 's crazy how much sexual attention can manipulate your brain . getting it from multiple women was blowing my ego out of the water and i could n't even see it happening . when i think back , i feel like toby maquire putting on the black \u001b[1m\u001b[94msuit\u001b[0m . it was wild . i would advise people not to follow in my footsteps , if they asked .\n","\n","\u001b[1m\u001b[91mdress\u001b[0m:\n","my mom wore a black \u001b[1m\u001b[91mdress\u001b[0m and vail when she married my dad . it was her 3rd marriage . her 2nd husband was killed in a hunting accident . my mom 's reasoning is that she was still in mourning for her second husband . i always found it disrespectful to my dad and their relationship .\n","\n","~~~\n","\u001b[1m\u001b[94mfilter\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7793443202972412\n","\u001b[1m\u001b[94mfilter\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated \u001b[1m\u001b[94mfilter\u001b[0m , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","hello corrupt_abyss . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow gift threads outside of stickied holiday posts . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for asking for gift advice \" & message =) . thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mre-read\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7788638472557068\n","\u001b[1m\u001b[94mre-read\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have \u001b[1m\u001b[94mre-read\u001b[0m your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","your comment has been removed because : your karma is too low to participate on askwomen . you will be able to participate when your karma has increased . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mtrade\u001b[0m & \u001b[1m\u001b[91meducation\u001b[0m, aligment score: 0.772612988948822\n","\u001b[1m\u001b[94mtrade\u001b[0m:\n","> “ i expected more ” from the guy who has lived in foreign countries , has an advanced degree that only a ridiculously small percentage of people get , and from all other accounts is reasonably attractive . and who is extremely lonely and isolated . i would happily \u001b[1m\u001b[94mtrade\u001b[0m in my degree , my body and my past for some friends and a partner in my home town .\n","\n","\u001b[1m\u001b[91meducation\u001b[0m:\n","makeup . and the lack of \u001b[1m\u001b[91meducation\u001b[0m around how dirty the makeup industry actually is !\n","\n","~~~\n","\u001b[1m\u001b[94mcash\u001b[0m & \u001b[1m\u001b[91mdebt\u001b[0m, aligment score: 0.7705771923065186\n","\u001b[1m\u001b[94mcash\u001b[0m:\n","so for every day : keys and a respectable mens wallet . enough \u001b[1m\u001b[94mcash\u001b[0m on you to cover a normal lunch and drinks after work or a date at a typical restaurant ( maybe $ 100 ) . credit card with no or very low balance so you can handle that unexpected vehicle breakdown . ( some one else posted that you should have an escape plan and this would be part of that . ) some kind of pocket knife or multitool . cell phone for home : a firearm that you are proficient with . home defense : 12 gauge shotgun , ar15 , 9mm semi auto in that order and arranged in your house so that you can get to at least one of them from anywhere in your house . a concealed handgun permit . you do n't have to carry , but that training is great . self defense insurance . a fully funded retirement plan ( if your company matches 5 % , if you kick in 5 % , then do that ) a savings account with 6 months of expenses for shtf time in your life . a nice bed that your gf / wife enjoys . enough tp to get through a month ( and again that your gf / wife approves of ) fem hygiene products and condoms in adequate quantities . fiber internet access if available so you can cord cut and she can watch a movie and you can work from home easily .\n","\n","\u001b[1m\u001b[91mdebt\u001b[0m:\n","if you have assets before marriage , get a prenup . romance will not save you from life . yes , it 's nice in cases of divorce to have the basics already outlined . as long as those items are enforceable , it streamlines the breakup process because you already know how things are divided . but it 's even better in situations outside of divorce ! prenups protect all parties in cases of death too ! if you have kids from outside of the current prenup marriage , you can define the division of your estate to make sure they get the pieces of your estate you want them to have . ex , you and your partner work high paying jobs and you want to give your kids an inheritance outside of state default division of property . you can guarantee your partner an amount and set the rest to be divided evenly amongst the rest of your kids , and it 'll be agreed to by all parties so in the height of grief there 's no fighting over it all . by defining property and account ownership in the prenup , you can limit the amount of marital property debtors can go after in the event of death . you can also set it up so certain types of expected \u001b[1m\u001b[91mdebt\u001b[0m are left only to the spouse pulling it and define whose income is responsible for getting it paid . ex , your spouse takes out student loans to go to medical school after marriage , those student loans are solely their responsibility as long as you do n't cosign for them , and debtors can't come after you and your personal assets or income upon their death . i get the gut check of \" he 's just thinking about how to leave me \" , but seriously , do the prenup . you can include as much or as little as you want to .\n","\n","~~~\n","\u001b[1m\u001b[94mknees\u001b[0m & \u001b[1m\u001b[91marm\u001b[0m, aligment score: 0.7678806185722351\n","\u001b[1m\u001b[94mknees\u001b[0m:\n","i have the \u001b[1m\u001b[94mknees\u001b[0m of a 20 years old now that i take collagen every day . ymmv and all , but just saying\n","\n","\u001b[1m\u001b[91marm\u001b[0m:\n","huge spiders i cant be in the same room . small spiders they can roam around and i wont care as long as it does n't come \u001b[1m\u001b[91marm\u001b[0m distance to me\n","\n","~~~\n","\u001b[1m\u001b[94mpiss\u001b[0m & \u001b[1m\u001b[91mpee\u001b[0m, aligment score: 0.7659830451011658\n","\u001b[1m\u001b[94mpiss\u001b[0m:\n","i always wash after # 2 , sometimes after # 1 . i also always wash my hands before i eat or prepare food . little kids are contagion vectors , and if i had my way , they 'd all be dipped in sterilant at regular intervals . or better yet , kept in quarantine somewhere where i don ' t have to hear them . if my body is clean , and i have no communicable diseases , and i do n't \u001b[1m\u001b[94mpiss\u001b[0m on myself ( i would definitely wash if i did ) , why should it matter ?\n","\n","\u001b[1m\u001b[91mpee\u001b[0m:\n","having sex and he said he was gonna cum and i wanted to swallow . he pulls out and i 'm swallowing + sucking cos i know that extra sensitivity can take things up a notch . he kinda tried to stop me but i 'm being playful and keep going ( about to wrap it up ) and suddenly i 'm not tasting cum anymore :loudly_crying_face: it was \u001b[1m\u001b[91mpee\u001b[0m ... we laughed about it right after and can til this day but whew . not a fun taste . :skull:\n","\n","~~~\n"]}],"source":["# MY DATASET\n","\n","for pair, score in sorted(zip(translations, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    word_a_highlighted = '{}{}{}'.format(color.BOLD + color.BLUE, pair[0], color.END)\n","    word_b_highlighted = '{}{}{}'.format(color.BOLD + color.RED, pair[1], color.END)\n","    print(word_a_highlighted + ' & ' + word_b_highlighted + ', aligment score: {}'.format(score))\n","    \n","    # print word_a_highlighted, and example sentences\n","    print('{}:'.format(word_a_highlighted))\n","    [print(s.replace(' {} '.format(pair[0]), ' {} '.format(word_a_highlighted))) for s in get_example_sentences(pair[0], corpus_a, n_sentences=n_sentences, seed=seed)]\n","    \n","    # print word_b_highlighted, and example sentences\n","    print('{}:'.format(word_b_highlighted))\n","    [print(s.replace(' {} '.format(pair[1]), ' {} '.format(word_b_highlighted))) for s in get_example_sentences(pair[1], corpus_b, n_sentences=n_sentences, seed=seed)]\n","    \n","    print('~~~')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2B5rB1AdMi-"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"Analysis.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}