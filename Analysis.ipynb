{"cells":[{"cell_type":"markdown","source":["# On colab, switch to GPU (Edit>>Notebook settings>>Hardware Accelerator), not CPU. Is >12x faster (eg1. for align_cca, GPU 20s, CPU >240s and it still hadnt finished running; eg2. for step 5A train_model.py, GPU 104s faster by 2x, CPU 191s)"],"metadata":{"id":"BVEqGQEurvNy"}},{"cell_type":"markdown","source":["# To wrap cell outputs"],"metadata":{"id":"OXQ3s_6XzgAv"}},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"jMA4X-5Vzdtp","executionInfo":{"status":"ok","timestamp":1650270843651,"user_tz":-480,"elapsed":523,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwHlab73dMic","executionInfo":{"status":"ok","timestamp":1650268421207,"user_tz":-480,"elapsed":655,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"76ec7791-2a5e-4322-ec73-bea0c02d079e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pickle\n","from tqdm import tqdm\n","import os\n","\n","from gensim.models.word2vec import Word2Vec\n","import numpy as np\n","from sklearn.cross_decomposition import CCA\n","from sklearn.metrics.pairwise import cosine_similarity\n","import json\n","from nltk.corpus import stopwords, wordnet\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","import random"]},{"cell_type":"code","source":["# install new packages\n","!pip install translators --upgrade;\n","!pip install langdetect;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-pXc_0jhQdN","executionInfo":{"status":"ok","timestamp":1650268430595,"user_tz":-480,"elapsed":8995,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"f6907075-6880-4242-8364-d26e61e1b2f3"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: translators in /usr/local/lib/python3.7/dist-packages (5.0.2)\n","Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from translators) (4.8.0)\n","Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.7/dist-packages (from translators) (2.27.1)\n","Requirement already satisfied: pathos>=0.2.8 in /usr/local/lib/python3.7/dist-packages (from translators) (0.2.8)\n","Requirement already satisfied: loguru>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from translators) (0.6.0)\n","Requirement already satisfied: PyExecJS>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from translators) (1.5.1)\n","Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.0)\n","Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.70.12.2)\n","Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.4)\n","Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (1.6.6.4)\n","Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from ppft>=1.6.6.4->pathos>=0.2.8->translators) (1.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2021.10.8)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.0.12)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"N4b4_zg3dMik"},"source":["# Worldview & Ideology Analysis"]},{"cell_type":"markdown","metadata":{"id":"kUIlosmXdMin"},"source":["This notebook contains examples of how to perform the analysis from \"Aligning Multidimensional Worldviews and Discovering Ideological Differences\" (Milbauer et al., 2021)"]},{"cell_type":"markdown","metadata":{"id":"Wwkv6QTGdMio"},"source":["## If editing in Google Colab: mount gdrive, clone github repo (if not already cloned), and go to cloned folder"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZh9EyxgdMip","executionInfo":{"status":"ok","timestamp":1650268433403,"user_tz":-480,"elapsed":2818,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"59dfae49-2ef9-4c08-c2e6-b32904d088a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount gdrive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5DaogstdMip","executionInfo":{"status":"ok","timestamp":1650268433405,"user_tz":-480,"elapsed":43,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"e84b54b8-4070-4e2d-e41e-28f33f48bd8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Colab_Notebooks/worldview-ideology/'\n","/content/drive/MyDrive/Colab_Notebooks/worldview-ideology\n","Analysis.ipynb\tdata\t    preprocessing.py  src\tutils.py\n","corpus\t\texample.sh  __pycache__       train.py\n"]}],"source":["# %cd drive/MyDrive/Colab_Notebooks #go to main Colab folder \n","# ! git clone 'https://github.com/limyuzheng88/worldview-ideology.git' #clone\n","# ! git pull #if already cloned repo, then just pull\n","\n","# To commit and push them to the repository, from within a Colab notebook: click File → Save a copy in GitHub .\n","\n","%cd drive/MyDrive/Colab_Notebooks/worldview-ideology/\n","!ls"]},{"cell_type":"code","source":["# !git init\n","# !git config --global user.email 'limyuzheng88@yahoo.com.sg'\n","# !git config --global user.name 'limyuzheng88'\n","# !git add .\n","# !git status\n","# !git commit -m '1. updated Analysis.ipynb by integrating bash scripts into it as py scripts; 2. added new word2vec models'\n","\n","# !git remote remove origin \n","# !git remote add origin https://limyuzheng88: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@github.com/limyuzheng88/worldview-ideology.git\n","!git push -f origin master\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"5ShOmKH31qAs","executionInfo":{"status":"ok","timestamp":1650272621250,"user_tz":-480,"elapsed":919,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"a3e7ed9e-fa59-4759-da3f-ea7564e6a9cd"},"execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["To https://github.com/limyuzheng88/worldview-ideology.git\n"," ! [rejected]        master -> master (fetch first)\n","error: failed to push some refs to 'https://limyuzheng88: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@github.com/limyuzheng88/worldview-ideology.git'\n","hint: Updates were rejected because the remote contains work that you do\n","hint: not have locally. This is usually caused by another repository pushing\n","hint: to the same ref. You may want to first integrate the remote changes\n","hint: (e.g., 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"p5hyRDyV4kgq","executionInfo":{"status":"ok","timestamp":1650272131418,"user_tz":-480,"elapsed":371,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"e4e5cbfb-77d5-47f5-b4d6-42b59d59df77"},"execution_count":87,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab_Notebooks/worldview-ideology'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"2cUZOKRZdMiq"},"source":["## Load corpora"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iYctBuIudMir","executionInfo":{"status":"ok","timestamp":1650268207204,"user_tz":-480,"elapsed":3967,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["source_a, source_b = 'subreddit_askmen', 'subreddit_askwomen'\n","with open('./corpus/raw/{}.txt'.format(source_a), encoding='utf-8') as f:\n","    corpus_a = f.readlines()\n","with open('./corpus/raw/{}.txt'.format(source_b), encoding='utf-8') as f:\n","    corpus_b = f.readlines()"]},{"cell_type":"code","source":["import translators as ts\n","from langdetect import detect\n","\n","print(corpus_a[0])\n","print(ts.google(corpus_a[0], from_language='en', to_language='zh-CN'))\n","print(ts.google('i pay for this river bank', from_language='en', to_language='zh-CN')) #test ambiguous translations\n","detect('狗屎。 总是一个条件 https://google.com')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"cd91wdLVh1-D","executionInfo":{"status":"ok","timestamp":1650268216455,"user_tz":-480,"elapsed":4089,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"fce12cf9-f64c-48b0-e951-26bc2351d648"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Using United States server backend.\n"]},{"output_type":"stream","name":"stdout","text":["shit . always a condition .\n","\n","狗屎。 总是一个条件。\n","我支付这条河岸\n"]},{"output_type":"execute_result","data":{"text/plain":["'zh-cn'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Preprocess text, run scripts"],"metadata":{"id":"_h2EywDPP0Cq"}},{"cell_type":"code","source":["# # 1. process the raw tokenized data into prepared phrases\n","# !python preprocessing.py corpus/raw/ corpus/prep/;\n","\n","# # 2. compute the word frequencies for each worldview file. This will give you the shared vocabularies to use for alignment\n","# !python ./src/stats/counts.py ./corpus ./data/counts.json\n","\n","# a general-purpose embedding to use for word clustering. You need to choose a text file that best represents \"generic\" language among your communities. You could use the union of the worldview files. That's what we did, and the method implemented by multitrain.sh\n","# 3. First, build the general-purpose embedding:\n","DIMS=100\n","WINDOW=5\n","SAMPLE='.00001'\n","# !python ./src/modeling/train_model.py ./corpus/raw ./data/master.model $DIMS $WINDOW $SAMPLE \"multi\"\n","\n","# # 4. Then, compute the clusters: \n","# !python ./src/stats/topics.py ./data/master.model ./data/clusters.json\n","\n","# 5. Place your preprocessed text file, each representing one worldview, into corpus/. Then\n","src = './corpus'\n","\n","# 5A. train all files in ./corpus...\n","for filename in tqdm(os.listdir(src)):\n","  if '.txt' in filename: #only train the datasets (which shall be .txt files)\n","    src_filepath = '{}/raw/{}'.format(src, filename)\n","    target='./data/models/{}.model'.format(''.join(filename.split('.')[:-1]))\n","    !python ./src/modeling/train_model.py $src_filepath $target $DIMS $WINDOW $SAMPLE\n","\n","# # 5B. or train files individually in ./corpus...\n","# for source in tqdm([source_a, source_b]): #edit list as desired, to desired filenames (without extension)\n","#   src_filepath = '{}/prep/{}.txt'.format(src, source)\n","#   target = './data/models/{}.word2vec.model'.format(source)\n","#   !python ./train.py $src_filepath $target;\n","\n","# # Run one of the following aligners:\n","# python ./src/alignment/align.py ./data/models/ ./data/aligners/cca/ ./data/counts.json cca 1000\n","# # python ./src/alignment/align.py ./data/models/ ./data/aligners/svd/ ./data/counts.json svd 1000 # (Khudabukhsh, et al.)\n","# # python ./src/alignment/align.py ./data/models/ ./data/aligners/lstsq/ ./data/counts.json lstsq 1000\n","\n","# # Check out each of the analysis notebooks for ideas."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLDrSxrPrznp","executionInfo":{"status":"ok","timestamp":1650270453650,"user_tz":-480,"elapsed":138081,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"995b98f6-de8f-41c1-fae0-294bc5a82471"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/7 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO:gensim.models.word2vec:collecting all words and their counts\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 400515 words, keeping 16900 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 801589 words, keeping 24059 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 1178090 words, keeping 29011 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 1613294 words, keeping 33666 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1996052 words, keeping 37521 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 2376003 words, keeping 40708 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 2836696 words, keeping 44105 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 3251903 words, keeping 46922 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 3653809 words, keeping 49519 word types\n","INFO:gensim.models.word2vec:collected 50524 word types from a corpus of 3806477 raw words and 93717 sentences\n","INFO:gensim.models.word2vec:max_final_vocab=15000 and min_count=100 resulted in calc_min_count=6, effective_min_count=100\n","INFO:gensim.models.word2vec:Loading a fresh vocabulary\n","INFO:gensim.models.word2vec:effective_min_count=100 retains 2287 unique words (4% of original 50524, drops 48237)\n","INFO:gensim.models.word2vec:effective_min_count=100 leaves 3480031 word corpus (91% of original 3806477, drops 326446)\n","INFO:gensim.models.word2vec:deleting the raw counts dictionary of 50524 items\n","INFO:gensim.models.word2vec:sample=1e-05 downsamples 2287 most-common words\n","INFO:gensim.models.word2vec:downsampling leaves estimated 421184 word corpus (12.1% of prior 3480031)\n","INFO:gensim.models.base_any2vec:estimated required memory for 2287 words and 100 dimensions: 2973100 bytes\n","INFO:gensim.models.word2vec:resetting layer weights\n","INFO:gensim.models.base_any2vec:training model with 2 workers on 2287 vocabulary and 100 features, using sg=1 hs=0 sample=1e-05 negative=5 window=5\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 36.97% examples, 152900 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 69.70% examples, 144268 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 3806477 raw words (421347 effective words) took 2.9s, 146303 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 36.44% examples, 148766 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 73.60% examples, 153020 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 3806477 raw words (420914 effective words) took 2.7s, 154899 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 38.02% examples, 157263 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 75.06% examples, 156942 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 3806477 raw words (421489 effective words) took 2.7s, 158199 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 38.26% examples, 158456 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 75.35% examples, 158291 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 3806477 raw words (421920 effective words) took 2.7s, 158924 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 37.76% examples, 156609 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 74.83% examples, 156800 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 3806477 raw words (421947 effective words) took 2.7s, 159101 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 38.02% examples, 156136 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 75.12% examples, 157032 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 3806477 raw words (421862 effective words) took 2.7s, 158202 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 37.76% examples, 155981 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 75.12% examples, 157777 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 3806477 raw words (422380 effective words) took 2.7s, 158840 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 38.02% examples, 156262 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 76.20% examples, 158254 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 3806477 raw words (420312 effective words) took 2.6s, 160289 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 38.02% examples, 157199 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 75.91% examples, 158922 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 9 : training on 3806477 raw words (421277 effective words) took 2.6s, 159728 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 38.48% examples, 157938 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 76.20% examples, 158813 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 10 : training on 3806477 raw words (421282 effective words) took 2.6s, 159785 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 38.26% examples, 158043 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 76.38% examples, 159690 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 11 : training on 3806477 raw words (421426 effective words) took 2.6s, 160368 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 38.02% examples, 155729 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 75.91% examples, 157875 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 12 : training on 3806477 raw words (420450 effective words) took 2.6s, 159927 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 38.02% examples, 157068 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 75.91% examples, 159124 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 13 : training on 3806477 raw words (420672 effective words) took 2.6s, 158932 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 37.76% examples, 155732 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 75.35% examples, 157658 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 14 : training on 3806477 raw words (421471 effective words) took 2.7s, 157918 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 37.24% examples, 152682 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 74.83% examples, 156186 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 15 : training on 3806477 raw words (420573 effective words) took 2.7s, 157047 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 37.52% examples, 154665 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 74.83% examples, 156439 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 16 : training on 3806477 raw words (420946 effective words) took 2.7s, 157897 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 37.76% examples, 155800 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 75.12% examples, 156458 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 17 : training on 3806477 raw words (420676 effective words) took 2.7s, 157868 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 36.97% examples, 152347 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 73.85% examples, 154272 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 18 : training on 3806477 raw words (420963 effective words) took 2.7s, 155647 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 38.26% examples, 157675 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 75.91% examples, 158618 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 19 : training on 3806477 raw words (421387 effective words) took 2.6s, 159608 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 38.26% examples, 158801 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 75.91% examples, 159302 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 20 : training on 3806477 raw words (421317 effective words) took 2.6s, 159487 effective words/s\n","INFO:gensim.models.base_any2vec:training on a 76129540 raw words (8424611 effective words) took 53.4s, 157828 effective words/s\n","INFO:gensim.utils:saving Word2Vec object under data/models/subreddit_askmen.model, separately None\n","INFO:gensim.utils:not storing attribute vectors_norm\n","INFO:gensim.utils:not storing attribute cum_table\n","INFO:gensim.utils:saved data/models/subreddit_askmen.model\n","INFO:root:Completed embedding from: subreddit_askmen\n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 6/7 [00:57<00:09,  9.50s/it]"]},{"output_type":"stream","name":"stdout","text":["INFO:gensim.models.word2vec:collecting all words and their counts\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 549272 words, keeping 17616 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 1079859 words, keeping 24703 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 1608392 words, keeping 30241 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 2148660 words, keeping 34641 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 2705188 words, keeping 38614 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 3231817 words, keeping 42279 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 3768673 words, keeping 45575 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 4311631 words, keeping 48613 word types\n","INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 4885894 words, keeping 51572 word types\n","INFO:gensim.models.word2vec:collected 52469 word types from a corpus of 5068738 raw words and 93206 sentences\n","INFO:gensim.models.word2vec:max_final_vocab=15000 and min_count=100 resulted in calc_min_count=6, effective_min_count=100\n","INFO:gensim.models.word2vec:Loading a fresh vocabulary\n","INFO:gensim.models.word2vec:effective_min_count=100 retains 2616 unique words (4% of original 52469, drops 49853)\n","INFO:gensim.models.word2vec:effective_min_count=100 leaves 4724927 word corpus (93% of original 5068738, drops 343811)\n","INFO:gensim.models.word2vec:deleting the raw counts dictionary of 52469 items\n","INFO:gensim.models.word2vec:sample=1e-05 downsamples 2259 most-common words\n","INFO:gensim.models.word2vec:downsampling leaves estimated 602249 word corpus (12.7% of prior 4724927)\n","INFO:gensim.models.base_any2vec:estimated required memory for 2616 words and 100 dimensions: 3400800 bytes\n","INFO:gensim.models.word2vec:resetting layer weights\n","INFO:gensim.models.base_any2vec:training model with 2 workers on 2616 vocabulary and 100 features, using sg=1 hs=0 sample=1e-05 negative=5 window=5\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 25.76% examples, 153393 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 51.59% examples, 154087 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 78.47% examples, 154833 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 5068738 raw words (602223 effective words) took 3.9s, 155591 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 25.99% examples, 154009 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 52.21% examples, 155950 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 79.15% examples, 156986 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 5068738 raw words (602682 effective words) took 3.8s, 157017 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 25.00% examples, 149186 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 51.20% examples, 153111 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 78.47% examples, 154916 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 5068738 raw words (601658 effective words) took 3.9s, 155681 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 25.76% examples, 153760 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 51.97% examples, 155424 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 78.66% examples, 155393 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 5068738 raw words (602889 effective words) took 3.9s, 155628 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 25.15% examples, 149439 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 50.64% examples, 151417 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 76.12% examples, 150501 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 5068738 raw words (603647 effective words) took 4.0s, 152611 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 25.57% examples, 152699 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 51.79% examples, 155214 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 79.52% examples, 157527 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 5068738 raw words (601942 effective words) took 3.8s, 158090 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 25.76% examples, 154111 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 52.42% examples, 156706 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 79.52% examples, 157647 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 5068738 raw words (602476 effective words) took 3.8s, 157992 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 25.76% examples, 154241 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 52.42% examples, 156598 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 79.52% examples, 157571 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 5068738 raw words (601984 effective words) took 3.8s, 157980 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 26.22% examples, 155953 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 52.87% examples, 157531 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 9 - PROGRESS: at 79.78% examples, 157946 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 9 : training on 5068738 raw words (602178 effective words) took 3.8s, 158613 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 25.76% examples, 154133 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 52.42% examples, 156704 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 10 - PROGRESS: at 79.52% examples, 157203 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 10 : training on 5068738 raw words (601988 effective words) took 3.8s, 157799 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 24.13% examples, 144837 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 46.85% examples, 140155 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 73.94% examples, 147231 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 11 - PROGRESS: at 99.39% examples, 149117 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 11 : training on 5068738 raw words (602103 effective words) took 4.0s, 149292 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 25.99% examples, 154366 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 52.42% examples, 155874 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 12 - PROGRESS: at 79.37% examples, 157000 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 12 : training on 5068738 raw words (602776 effective words) took 3.9s, 156558 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 25.57% examples, 151869 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 47.02% examples, 139935 words/s, in_qsize 2, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 73.34% examples, 145376 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 13 - PROGRESS: at 98.97% examples, 148393 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 13 : training on 5068738 raw words (602163 effective words) took 4.0s, 148919 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 25.76% examples, 153846 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 51.97% examples, 155636 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 14 - PROGRESS: at 79.15% examples, 156882 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 14 : training on 5068738 raw words (602342 effective words) took 3.8s, 157415 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 25.76% examples, 153545 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 52.42% examples, 155820 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 15 - PROGRESS: at 79.52% examples, 156797 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 15 : training on 5068738 raw words (601488 effective words) took 3.8s, 156982 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 25.76% examples, 153508 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 51.97% examples, 155810 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 16 - PROGRESS: at 78.96% examples, 156698 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 16 : training on 5068738 raw words (602896 effective words) took 3.8s, 158213 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 26.22% examples, 155552 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 52.42% examples, 155656 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 17 - PROGRESS: at 78.96% examples, 156056 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 17 : training on 5068738 raw words (602724 effective words) took 3.8s, 156745 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 25.99% examples, 154125 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 52.21% examples, 155603 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 18 - PROGRESS: at 79.15% examples, 156630 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 18 : training on 5068738 raw words (602080 effective words) took 3.8s, 156911 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 25.32% examples, 151694 words/s, in_qsize 3, out_qsize 1\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 51.79% examples, 154547 words/s, in_qsize 4, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 19 - PROGRESS: at 78.96% examples, 156180 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 19 : training on 5068738 raw words (601882 effective words) took 3.8s, 157130 effective words/s\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 25.99% examples, 154319 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 52.21% examples, 155590 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:EPOCH 20 - PROGRESS: at 79.37% examples, 156878 words/s, in_qsize 3, out_qsize 0\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n","INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n","INFO:gensim.models.base_any2vec:EPOCH - 20 : training on 5068738 raw words (601335 effective words) took 3.8s, 157671 effective words/s\n","INFO:gensim.models.base_any2vec:training on a 101374760 raw words (12045456 effective words) took 77.2s, 156061 effective words/s\n","INFO:gensim.utils:saving Word2Vec object under data/models/subreddit_askwomen.model, separately None\n","INFO:gensim.utils:not storing attribute vectors_norm\n","INFO:gensim.utils:not storing attribute cum_table\n","INFO:gensim.utils:saved data/models/subreddit_askwomen.model\n","INFO:root:Completed embedding from: subreddit_askwomen\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [02:17<00:00, 19.71s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"I5G6j3oJdMis"},"source":["## Loading the trained embeddings"]},{"cell_type":"markdown","metadata":{"id":"lHHb5O_SdMit"},"source":["First, we load the trained embeddings, and quickly examine them to see if they make sense.\n","We are using small text samples (500k tokens), so embeddings may not be very good."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq2SihsrdMiu","outputId":"084689ac-32df-46ff-fa33-091131a4ab9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["('republican', 0.6307386159896851)\n","('dem', 0.6165011525154114)\n","('democrats', 0.6043610572814941)\n","('democratic', 0.5978788137435913)\n","('dems', 0.49818018078804016)\n","('liberal', 0.48974037170410156)\n","('candidate', 0.4867892265319824)\n","('party', 0.4780939519405365)\n","('republicans', 0.477538526058197)\n","('progressive', 0.4756127595901489)\n","\n","('dem', 0.6980773210525513)\n","('republican', 0.6605644226074219)\n","('democratic', 0.6189409494400024)\n","('democrats', 0.6140726804733276)\n","('party', 0.6009922623634338)\n","('dems', 0.48170965909957886)\n","('liberal', 0.4369395971298218)\n","('leftist', 0.42886313796043396)\n","('left', 0.4251934587955475)\n","('candidates', 0.4231654405593872)\n"]}],"source":["model_a = Word2Vec.load('models/politics.word2vec.model')\n","model_b = Word2Vec.load('models/the_donald.word2vec.model')\n","# pretrained on more data\n","# model_a = Word2Vec.load('models/politics.big.model')\n","# model_b = Word2Vec.load('models/the_donald.big.model')\n","\n","posWords = ['democrat']\n","negWords = []\n","for x in model_a.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)\n","print()\n","for x in model_b.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pE1x-B6dMiv","executionInfo":{"status":"ok","timestamp":1650270676583,"user_tz":-480,"elapsed":1128,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"2957a38a-7b0d-4915-9495-bd9675830cd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["('orgasm', 0.7038615942001343)\n","('sexual', 0.6980441212654114)\n","('horny', 0.6530160903930664)\n","('pleasure', 0.6499814987182617)\n","('intimacy', 0.6482909321784973)\n","('consent', 0.6474172472953796)\n","('oral', 0.6240077018737793)\n","('sexuality', 0.6177167296409607)\n","('wanting', 0.6117582321166992)\n","('porn', 0.6108335852622986)\n","\n","('control', 0.6555414199829102)\n","('birth', 0.6115919351577759)\n","('feminism', 0.611216127872467)\n","('pregnancy', 0.5445745587348938)\n","('birthcontrol', 0.5277174711227417)\n","('parenting', 0.5203535556793213)\n","('gifts', 0.5160411596298218)\n","('pleasure', 0.511846125125885)\n","('hair', 0.5033841133117676)\n","('malefashionadvice', 0.5019433498382568)\n"]}],"source":["# MY DATASET\n","\n","model_a = Word2Vec.load('data/models/{}.model'.format(source_a)) #or use the .word2vec.model (larger, cos trained on more epochs)\n","model_b = Word2Vec.load('data/models/{}.model'.format(source_b))\n","\n","posWords = ['men', 'sex']\n","negWords = ['women']\n","for x in model_a.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)\n","print()\n","for x in model_b.wv.most_similar(positive=posWords, negative=negWords):\n","    print(x)"]},{"cell_type":"markdown","metadata":{"id":"e39j4aELdMiw"},"source":["## Aligning the embeddings"]},{"cell_type":"markdown","metadata":{"id":"4jHjk6l0dMiw"},"source":["First, we find the overlapping vocabulary of the two models, and use this to construct an embedding matrix for each model."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"kb81tKDjdMix","executionInfo":{"status":"ok","timestamp":1650270686182,"user_tz":-480,"elapsed":421,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["vocab_a = list(set(model_a.wv.vocab.keys()))\n","vocab_b = list(set(model_b.wv.vocab.keys()))\n","\n","shared_vocab = set.intersection(set(vocab_a),\n","                                set(vocab_b))\n","shared_vocab = list(sorted(list(shared_vocab)))\n","combo_vocab = set.union(set(vocab_a),\n","                                set(vocab_b))\n","\n","w2idx = { w:i for i,w in enumerate(shared_vocab) }\n","a2idx = { w:i for i,w in enumerate(vocab_a) }\n","idx2b = { i:w for i,w in enumerate(vocab_b) }\n","\n","mtxA = np.vstack([model_a.wv[w] for w in shared_vocab])\n","mtxB = np.vstack([model_b.wv[w] for w in shared_vocab])\n","mtxA_ = np.vstack([model_a.wv[w] for w in vocab_a])\n","mtxB_ = np.vstack([model_b.wv[w] for w in vocab_b])"]},{"cell_type":"markdown","metadata":{"id":"-YaxZlcEdMiy"},"source":["We then select only the N most common words as anchors to train our alignment. (If you're using the big model, this won't quite work because the vocabularies are different.)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"RLWSEwkqdMiz","executionInfo":{"status":"ok","timestamp":1650270687686,"user_tz":-480,"elapsed":364,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["counts = pickle.load(open('data/counts.pkl', 'rb'))\n","n = 5000\n","topN = [y for x,y in sorted([(counts[w], w) for w in w2idx if w in counts], reverse=True)][:n] #w2idx is from shared_vocab\n","idxs = [w2idx[w] for w in topN]"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"uKNU65F2dMiz","executionInfo":{"status":"ok","timestamp":1650270688237,"user_tz":-480,"elapsed":3,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["anchorA = mtxA[idxs, :]\n","anchorB = mtxB[idxs, :]"]},{"cell_type":"markdown","metadata":{"id":"dZhgHzESdMi0"},"source":["Next, we use two different techniques for aligning the embeddings: SVD and CCA"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"nB7GnQCadMi0","executionInfo":{"status":"ok","timestamp":1650270690932,"user_tz":-480,"elapsed":355,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def align_svd(source, target):\n","    product = np.matmul(source.transpose(), target)\n","    U, s, V = np.linalg.svd(product)\n","    T = np.matmul(U,V)\n","    return T\n","\n","svd = align_svd(anchorA, anchorB)\n","svdA = mtxA_.dot(svd)\n","svdB = mtxB_"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"XjhIeI8PdMi1","executionInfo":{"status":"ok","timestamp":1650268747537,"user_tz":-480,"elapsed":18876,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def align_cca(source, target):\n","    N_dims = source.shape[1]\n","    cca = CCA(n_components=N_dims, max_iter=2000)\n","    cca.fit(source, target)\n","    return cca\n","\n","cca = align_cca(anchorA, anchorB)\n","ccaA, ccaB = cca.transform(mtxA, mtxB)"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"2uqxbALAdMi1","executionInfo":{"status":"ok","timestamp":1650270701790,"user_tz":-480,"elapsed":339,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["def build_translator(a, b, a2idx, idx2b):\n","    sims = cosine_similarity(a, b)\n","    most_sims = np.argsort(sims, axis=1)[:, ::-1]\n","    \n","    def translator(w, k=1):\n","        idx = a2idx[w]\n","        idxs = most_sims[idx, :k]\n","        words = [idx2b[i] for i in idxs]\n","        return words, sims[idx, idxs]\n","    \n","    return translator"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"-NV17SLKdMi1","executionInfo":{"status":"ok","timestamp":1650270703578,"user_tz":-480,"elapsed":549,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}}},"outputs":[],"source":["translator = build_translator(svdA, svdB, a2idx, idx2b)"]},{"cell_type":"markdown","metadata":{"id":"QNl7CfbzdMi2"},"source":["## Exploring the Alignment"]},{"cell_type":"markdown","metadata":{"id":"XKssZjy4dMi2"},"source":["We now explore three different ways of using the alignmed embeddings to explore the worldview and ideology of the two communities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnG_ASy2dMi3","outputId":"48fddb2d-e9d4-4a9a-f3ff-15d904848d55"},"outputs":[{"data":{"text/plain":["(['democrat', 'republican', 'dem', 'democrats', 'republicans'],\n"," array([0.6164709 , 0.5891098 , 0.5137549 , 0.4719857 , 0.46580008],\n","       dtype=float32))"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["translator('democrat', k=5)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6d5VJd0dMi3","executionInfo":{"status":"ok","timestamp":1650270720641,"user_tz":-480,"elapsed":339,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"73f006f3-307d-4207-b432-8d8e1e09c0dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['men', 'women', 'misogyny', 'themselves', 'trans'],\n"," array([0.7862248, 0.7762395, 0.6666307, 0.6360846, 0.6330702],\n","       dtype=float32))"]},"metadata":{},"execution_count":54}],"source":["# MY DATASET\n","translator('men', k=5)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":902},"id":"UywAIrxBdMi4","executionInfo":{"status":"ok","timestamp":1650270852984,"user_tz":-480,"elapsed":350,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"ee6ec3b5-6bb3-4975-82c7-b55dbf947923"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["'\u001b[1m\u001b[94msex\u001b[0m' in source_a (1 example sentence below):\n","i am 36 2 women so far , the first at the age of 26 . the last time i had \u001b[1m\u001b[94msex\u001b[0m was 2016 .\n","\n","translates to source_b words:\n","\n","\u001b[1m\u001b[91mpartners\u001b[0m, aligment score: 0.7187862992286682\n","op here . i 've noticed that when my friends text me , im excited and want to see what they 've said . when my \u001b[1m\u001b[91mpartners\u001b[0m text me , i feel obligated to respond . it 's a chore . or i feel like i have to keep up appearances and make sure i do n't respond too fast .\n","\n","\u001b[1m\u001b[91mintimacy\u001b[0m, aligment score: 0.7099165916442871\n","this is my parents exactly ! luckily my mom got fed up last year because she realized the baby if the family would be moving out this year . the only good thing she can say about my dad is that he takes care of things financially & he 's really good with babies , but other than that it 's negative . to be fair she ignored lots of signs , including verbally telling him they would n't work because their core values were different , but she was rolling up on 35 and wanted another child and did n't want to have 2 baby daddies . also my southern family kept having snide remarks about her being unmarried at her age ... ! ? so for 20 years she 's been in a relationship that has lacked \u001b[1m\u001b[91mintimacy\u001b[0m , affection , kindness , assistance , and love . their decent friends but it 's hard for me to look at marriage positively when the main one i 've known has seemed like a trap from the beginning ( my mom says all the time that although there were red flags , my dad did a complete 180 when they were actually married . to the point he even chewed different ) .\n","\n","\u001b[1m\u001b[91morgasm\u001b[0m, aligment score: 0.7019028067588806\n","yes ! exactly . this man i 've slept with recently literally thought i \u001b[1m\u001b[91morgasm\u001b[0med several times from his penetration alone . no my dude . things simply do not work that way for most women . myself included . i explained this and he did n't want to believe it and i do n't think he cares at all how wrong he is . such a turn off\n","\n","\u001b[1m\u001b[91msex\u001b[0m, aligment score: 0.6990844011306763\n","this comment or post has been removed for seeking advice about a specific person or personal situation . askwomen is about receiving answers from the community about their own opinions and experiences rather than seeking input on specific personal situations . * personal / life advice : try / r / askwomenadvice or / r / thegirlsurvivalguide . * dating / relationship advice : try / r / relationships . * personal pictures or social media : try r / firstimpressions or r / tinder . * medical advice : if you are seeking medical advice , please consult a physician or try / r / askdocs . * legal advice : try / r / legaladvice . * makeup or skincare advice : try / r / makeupaddiction or / r / skincareaddiction . * hair advice : try / r / hair or / r / fancyfollicles . * fashion advice : try / r / femalefashionadvice or / r / malefashionadvice . * birth control advice : try / r / birthcontrol or / r / askdocs . * gift advice : try / r / gifts . * \u001b[1m\u001b[91msex\u001b[0m advice : try / r / \u001b[1m\u001b[91msex\u001b[0m . * feminism : try / r / askfeministwomen * parenting / pregnancy advice : try / r / babybumps , / r / beyondthebump , or / r / askparents . * debate about your views : try / r / changemyview * still trying to find the right sub : try / r / findareddit can help for safe-for-work posts or / r / nsfw 411 for nsfw posts # * * please review the rules of any sub before you post there . * * if you have any questions about this moderation action , please message the moderators through the link on the sidebar or [ here ] ( HTTPURL ) . if you are messaging about your removed comment or post , please include a link to the removed content for review .\n","\n","\u001b[1m\u001b[91mpleasure\u001b[0m, aligment score: 0.6903225779533386\n","this is not true . there are very few things in life that can be boiled down to “ women like x and men prefer y , ” everyone 's sexual preferences are going to be unique . i think for many people , sex gets better as you get older — you generally have a better sense of what you like and what you do n't , and you 're more comfortable advocating for your own \u001b[1m\u001b[91mpleasure\u001b[0m . several close friends of mine waited until marriage ( i did n't ) and those friends had very rough transitions to intimate life with their new spouses . waiting until the wedding night puts so much pressure on that first experience , which can often be awkward and even a little painful for the most committed couples . often times , people 's expectations about a magical movie-like orgasm on the first try are shattered . sex is always better when there 's no kind of external pressures involved .\n","\n"]}],"source":["# MY DATASET\n","class color:\n","   PURPLE = '\\033[95m'\n","   CYAN = '\\033[96m'\n","   DARKCYAN = '\\033[36m'\n","   BLUE = '\\033[94m'\n","   GREEN = '\\033[92m'\n","   YELLOW = '\\033[93m'\n","   RED = '\\033[91m'\n","   BOLD = '\\033[1m'\n","   UNDERLINE = '\\033[4m'\n","   END = '\\033[0m'\n","\n","# function to get example sentences that contain said word  \n","def get_example_sentences(word, corpus, n_sentences=1, seed=None):\n","    random.seed(a=seed) #fix seed if needed. removes randomness, and fixes the output as a constant\n","    sentences = random.sample([s for s in corpus if word in s], n_sentences)\n","    return sentences\n","    \n","n_sentences = 1\n","seed = None\n","\n","word_a = 'sex'\n","word_a_highlighted = '{}{}{}'.format(color.BOLD + color.BLUE, word_a, color.END)\n","print('{} in source_a ({} example sentence below):'.format(word_a_highlighted, n_sentences))\n","[print(s.replace(word_a, word_a_highlighted)) for s in get_example_sentences(word_a, corpus_a, n_sentences=n_sentences, seed=seed)]\n","\n","word_b = translator(word_a, k=5)\n","print('translates to source_b words:\\n')\n","for w in zip(translator(word_a, k=5)[0], translator(word_a, k=5)[1]):\n","    word_b_highlighted = '{}{}{}'.format(color.BOLD + color.RED, w[0], color.END)\n","    print(word_b_highlighted + ', aligment score: {}'.format(w[1]))\n","    [print(s.replace(w[0], word_b_highlighted)) for s in get_example_sentences(w[0], corpus_b, n_sentences=n_sentences, seed=seed)]"]},{"cell_type":"markdown","metadata":{"id":"72IoJRuydMi5"},"source":["### Misalignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okYnJ571dMi5","outputId":"8f65a005-8e6a-4e9c-d1b0-2ecbdcd74e50"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3664901664145234\n"]}],"source":["misaligned = []\n","scores = []\n","\n","for w in shared_vocab:\n","    w_ = translator(w)[0][0]\n","    s = translator(w)[1][0]\n","    if w != w_:\n","        misaligned.append((w, w_))\n","        scores.append(s)\n","        \n","print(len(misaligned) / len(shared_vocab))"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"ZND1xzAJdMi5","executionInfo":{"status":"ok","timestamp":1650270921836,"user_tz":-480,"elapsed":543,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"96228d3a-beea-4cb5-b9b9-c7e47f2ad2a1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.3889156626506024\n"]}],"source":["# MY DATASET\n","misaligned = []\n","scores = []\n","\n","for w in shared_vocab:\n","    w_ = translator(w)[0][0]\n","    s = translator(w)[1][0]\n","    if w != w_:\n","        misaligned.append((w, w_))\n","        scores.append(s)\n","        \n","print(len(misaligned) / len(shared_vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_nHF7WjdMi6","outputId":"8d96917c-c0dd-4bf0-fa3c-02e5a17c2e6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["('performed_automatically', 'please_contact') 0.8923226\n","('moderators', 'please_contact') 0.8301286\n","('``', \"''\") 0.7827312\n","('&', 'gt') 0.74673975\n","('bot', 'performed_automatically') 0.7402881\n","(';', 'gt') 0.71963507\n","('though', 'but') 0.7046928\n","('citizenship_question', 'census') 0.68586487\n","('amp', ';') 0.68398106\n","('action', 'performed_automatically') 0.6676772\n","('couple', 'few') 0.6567316\n","('disagree', 'agree') 0.64628285\n","('dems', 'democrats') 0.6362802\n","('supreme_court', 'scotus') 0.61996275\n","('republican', 'democrat') 0.6085014\n","('dumb', 'stupid') 0.60647255\n","('26_times', 'lolita_express') 0.6013237\n","('capitalism', 'communism') 0.5988106\n","('jeffrey_epstein', 'epstein') 0.59700453\n","('illegal_immigrants', 'illegals') 0.5922674\n"]}],"source":["for pair, score in sorted(zip(misaligned, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"RLL76NDXdMi6","executionInfo":{"status":"ok","timestamp":1650270928091,"user_tz":-480,"elapsed":329,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"486029ca-4ee3-4c79-8f1e-c7c96a6c70cb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["('subreddit', 'compose') 0.9350627\n","('performed', 'compose') 0.932218\n","('automatically', 'compose') 0.92873514\n","('concerns', '=/') 0.91821444\n","('bot', 'compose') 0.91762775\n","('moderators', 'compose') 0.9035689\n","('action', 'compose') 0.8891935\n","('shorts', 'pants') 0.8832135\n","('message', 'moderators') 0.88136923\n","('shirts', 'jeans') 0.87320757\n","('[', ']') 0.86907256\n","('r', '=/') 0.8665351\n","('tea', 'coffee') 0.8596305\n","('expensive', 'cheaper') 0.85752565\n","('please', 'message') 0.8567974\n","('shower', 'wash') 0.85311604\n","('tight', 'pants') 0.8521388\n","('9', '5') 0.8500881\n","('19', '18') 0.84579027\n","('paying', 'pay') 0.845045\n"]}],"source":["# MY DATASET\n","for pair, score in sorted(zip(misaligned, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"markdown","metadata":{"id":"4Cyv_kpJdMi7"},"source":["### Antonyms"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"XTds9rLIdMi7","executionInfo":{"status":"ok","timestamp":1650270938953,"user_tz":-480,"elapsed":3244,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"08d87342-5165-4ef8-d42e-e2e192d6daf4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2828/2828 [00:02<00:00, 958.85it/s] \n"]}],"source":["def get_antonyms(vocab):\n","    antonyms = []\n","    for w in tqdm(vocab):\n","        for synset in wordnet.synsets(w):\n","            for lemma in synset.lemmas():\n","                if lemma.antonyms():\n","                    antonyms.append((w, lemma.antonyms()[0].name()))\n","    antonyms = set(antonyms)\n","    return antonyms\n","\n","antonyms = get_antonyms(combo_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYwNg1kddMi7","outputId":"c0adcbeb-3e04-4d2f-9ae2-7e1434f24620"},"outputs":[{"name":"stdout","output_type":"stream","text":["('civilian', 'military')\n","('decrease', 'increase')\n","('disagree', 'agree')\n","('disrespect', 'respect')\n","('illogical', 'logical')\n","('inaccurate', 'accurate')\n","('indirectly', 'directly')\n","('ineffective', 'effective')\n","('intolerant', 'tolerant')\n","('invalid', 'valid')\n","('liability', 'asset')\n","('sell', 'buy')\n","('sells', 'buy')\n","('unreasonable', 'reasonable')\n","('unwilling', 'willing')\n","('weakness', 'strength')\n","('west', 'east')\n"]}],"source":["for mPair in misaligned:\n","    if mPair in antonyms or (mPair[0], mPair[1]) in antonyms:\n","        print(mPair)"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"hFA8NWbOdMi8","executionInfo":{"status":"ok","timestamp":1650270943351,"user_tz":-480,"elapsed":349,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"25b0abb8-f49e-4b4a-a5dd-f9d5c2d68583"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["('children', 'parent')\n","('daughter', 'son')\n","('father', 'mother')\n","('late', 'early')\n","('male', 'female')\n","('more', 'less')\n","('particular', 'general')\n","('second', 'first')\n","('son', 'daughter')\n","('white', 'black')\n","('wife', 'husband')\n"]}],"source":["# MY DATASET\n","for mPair in misaligned:\n","    if mPair in antonyms or (mPair[0], mPair[1]) in antonyms:\n","        print(mPair)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1GGi9UbdMi8","outputId":"e7e01491-e3ca-4e3f-994c-aae31c74c862"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5847953216374269% misaligned pairs from 'misaligned', in 'antonyms' set\n"]}],"source":["print(\"{}% misaligned pairs from 'misaligned', in 'antonyms' set\".format(round(len([mPair for mPair in misaligned if mPair in antonyms or (mPair[0], mPair[1]) in antonyms])/len(misaligned)*100, 2)))"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"C6rDYftsdMi8","executionInfo":{"status":"ok","timestamp":1650271084275,"user_tz":-480,"elapsed":363,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"47844fc0-5d4d-46ac-ef84-a26a40a4ff4e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1.36% misaligned pairs from 'misaligned', in 'antonyms' set\n"]}],"source":["# MY DATASET\n","print(\"{}% misaligned pairs from 'misaligned', in 'antonyms' set\".format(round(len([mPair for mPair in misaligned if mPair in antonyms or (mPair[0], mPair[1]) in antonyms])/len(misaligned)*100, 2)))"]},{"cell_type":"markdown","metadata":{"id":"HckDQAJsdMi9"},"source":["### Translation / Conceptual Homomorphisms"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"JA6GQZf6dMi9","executionInfo":{"status":"ok","timestamp":1650271119273,"user_tz":-480,"elapsed":333,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"5b66b158-d7c1-4187-ca6c-3fd77688016f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["unique_vocab = []\n","for w in model_a.wv.vocab:\n","    if w not in model_b.wv.vocab:\n","        unique_vocab.append(w)"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"GGihCU8GdMi9","executionInfo":{"status":"ok","timestamp":1650271119835,"user_tz":-480,"elapsed":20,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"2416c57c-dafe-4a4b-a940-34ba6aa99666"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["translations = []\n","scores = []\n","for w in unique_vocab:\n","    t = translator(w)\n","    translations.append((w, t[0][0]))\n","    scores.append(t[1][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_tOyQVbdMi-","outputId":"fe6354d3-5572-4d9b-881b-07c939e9088f"},"outputs":[{"name":"stdout","output_type":"stream","text":["('instructions_provided', 'performed_automatically') 0.71877486\n","('permanent_ban', 'performed_automatically') 0.69331694\n","('rule_violations', 'performed_automatically') 0.63353837\n","('wishing_death/physical', 'performed_automatically') 0.594555\n","('fully_participate', 'please_contact') 0.5898004\n","('rulebreaking_content', 'performed_automatically') 0.5775635\n","('`_youtu.be', '`') 0.55210274\n","('spam_domain', 'performed_automatically') 0.5434005\n","('/r/politics_within', 'performed_automatically') 0.52550036\n","('troll_accusations', 'performed_automatically') 0.51061064\n","('whitelisting', 'performed_automatically') 0.4963802\n","('blatant_spam', 'performed_automatically') 0.48971322\n","('confederate_flag', 'flag') 0.48527563\n","('excluding_indians', 'persons') 0.48497242\n","('site_administrators', 'link_shortener') 0.48107997\n","('following_reason', 'submission') 0.48058963\n","('alan_dershowitz', 'epstein') 0.48009375\n","('drinking_water', 'water') 0.47866067\n","('breaking_channel', 'link_shortener') 0.47774062\n","('nonreputable_/', 'performed_automatically') 0.47719014\n"]}],"source":["for pair, score in sorted(zip(translations, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    print(pair, score)"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lz1sxSJudMi-","executionInfo":{"status":"ok","timestamp":1650271128813,"user_tz":-480,"elapsed":1488,"user":{"displayName":"lim yu zheng","userId":"17408866843157441974"}},"outputId":"4c0a5157-8860-46a3-f276-b259c7366b30"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m\u001b[94maskmen\u001b[0m & \u001b[1m\u001b[91m=/\u001b[0m, aligment score: 0.9201257228851318\n","\u001b[1m\u001b[94maskmen\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / \u001b[1m\u001b[94maskmen\u001b[0m ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91m=/\u001b[0m:\n","hello spongky . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to \u001b[1m\u001b[91m=/\u001b[0m r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mreasoning\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.8699803352355957\n","\u001b[1m\u001b[94mreasoning\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any \u001b[1m\u001b[94mreasoning\u001b[0m , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","hello survivorscrolls . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mtowel\u001b[0m & \u001b[1m\u001b[91mwash\u001b[0m, aligment score: 0.8527393937110901\n","\u001b[1m\u001b[94mtowel\u001b[0m:\n","nothing huge . hopefully she has a clean place , somewhere to sit and watch tv , maybe some snacks and drinks . if you are planning for bedroom stuff have a bed and make sure the area is clean and tidy . if he maybe sleeping over have an extra clean \u001b[1m\u001b[94mtowel\u001b[0m and a toothbrush .\n","\n","\u001b[1m\u001b[91mwash\u001b[0m:\n","happens here all the time . i pull my stuff out of the \u001b[1m\u001b[91mwash\u001b[0ming machine and it 's different colour .\n","\n","~~~\n","\u001b[1m\u001b[94mboots\u001b[0m & \u001b[1m\u001b[91mpair\u001b[0m, aligment score: 0.8300528526306152\n","\u001b[1m\u001b[94mboots\u001b[0m:\n","definitely taking my belt off and probably taking my \u001b[1m\u001b[94mboots\u001b[0m off . i used to wear steel-toe \u001b[1m\u001b[94mboots\u001b[0m all the time for a class , i kind of miss them just for that relief moment at the end of the day .\n","\n","\u001b[1m\u001b[91mpair\u001b[0m:\n","a good \u001b[1m\u001b[91mpair\u001b[0m of earrings\n","\n","~~~\n","\u001b[1m\u001b[94mtaller\u001b[0m & \u001b[1m\u001b[91mheight\u001b[0m, aligment score: 0.8048233389854431\n","\u001b[1m\u001b[94mtaller\u001b[0m:\n","6 at best , maybe a point or 2 higher if i were \u001b[1m\u001b[94mtaller\u001b[0m\n","\n","\u001b[1m\u001b[91mheight\u001b[0m:\n","under 5 ' & curvy here .. totally get this . no clothes are made for my \u001b[1m\u001b[91mheight\u001b[0m & build at all .\n","\n","~~~\n","\u001b[1m\u001b[94mlifting\u001b[0m & \u001b[1m\u001b[91mexercise\u001b[0m, aligment score: 0.8041852116584778\n","\u001b[1m\u001b[94mlifting\u001b[0m:\n","\u001b[1m\u001b[94mlifting\u001b[0m weights and seeing gradual improvement in physique\n","\n","\u001b[1m\u001b[91mexercise\u001b[0m:\n","joint and back pain relief and inflammation control . thanks autoimmune crap and arthritis ! lots of walking , jogging on good days , stretching , and body weight resistance \u001b[1m\u001b[91mexercise\u001b[0m . had a very physical job for a good part of my life . if i do n't keep moving i feel like death .\n","\n","~~~\n","\u001b[1m\u001b[94mpounds\u001b[0m & \u001b[1m\u001b[91mweight\u001b[0m, aligment score: 0.7994028925895691\n","\u001b[1m\u001b[94mpounds\u001b[0m:\n","calorie counting works , but the fastest results will come from seriously reducing your sugar intake . excess sugar in your bloodstream is poison , so your body quickly converts it to fat . i dropped 20 \u001b[1m\u001b[94mpounds\u001b[0m in a few weeks simply by switching from sodas to water . you 'll get similar results . sugar is the key .\n","\n","\u001b[1m\u001b[91mweight\u001b[0m:\n","mainly i just want to feel good in my clothes and look / dress nice . i 've been over\u001b[1m\u001b[91mweight\u001b[0m all my life and have dressed to cover up my body , and it 's just always felt awful . i know losing \u001b[1m\u001b[91mweight\u001b[0m will not get rid of my insecurities 100 % but i feel better knowing that i am working to maintain a nice figure and take care of myself . i 'm in my early 20s , i want to spend them feeling confident in my body and my clothes . working out regularly also just makes me feel like i 'm investing in my future physical health . i sit most of the day so it is really important for me to get my body moving everyday . i feel happier and get great sleep because of it .\n","\n","~~~\n","\u001b[1m\u001b[94mautomated\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7986434102058411\n","\u001b[1m\u001b[94mautomated\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the \u001b[1m\u001b[94mautomated\u001b[0m filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","hello scienceisfun 321 . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mflagged\u001b[0m & \u001b[1m\u001b[91mmoderators\u001b[0m, aligment score: 0.7932913899421692\n","\u001b[1m\u001b[94mflagged\u001b[0m:\n","your post has been \u001b[1m\u001b[94mflagged\u001b[0m as a commonly asked topic here ( relationship break-ups ) . if you think this action was made in error , [ message our moderators ] ( HTTPURL ) with a link to the post for approval . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mmoderators\u001b[0m:\n","your submission has been removed because this question contains a post title or topic that is already active or commonly discussed . we are not accepting any new submissions regarding this post / topic this time so please search the subreddit using keywords before reposting . you can view our faq [ here ] ( HTTPURL ) or search the subreddit . you can always go to r / findareddit to find the right place for your post . if you think you received this message in error , copy & paste a link your post & [ click here to contact modmail . ] ( HTTPURL ) . please include the reason you are contesting this post and a link to your message . * i am a bot , and this action was performed automatically . please [ contact the \u001b[1m\u001b[91mmoderators\u001b[0m of this subreddit ] ( / message / compose / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94msalary\u001b[0m & \u001b[1m\u001b[91mpaid\u001b[0m, aligment score: 0.7878615856170654\n","\u001b[1m\u001b[94msalary\u001b[0m:\n","i live in france , probably one of world 's most unionised country , working in an already extremely unionised field and in the biggest ( and maybe oldest ) french company in that field . so yeah you can't get more unionised than that . we have a lot of other benefits but the \u001b[1m\u001b[94msalary\u001b[0m is definitely not one of them , and i would gladly move to the us and work there .\n","\n","\u001b[1m\u001b[91mpaid\u001b[0m:\n","i do not care . i already took on my share of charity cases and i \u001b[1m\u001b[91mpaid\u001b[0m . additionally , i 'm faaaat , therefore i do n't fit the standards of the people who have this mindset more often than not , so this benefits all of us .\n","\n","~~~\n","\u001b[1m\u001b[94mdollars\u001b[0m & \u001b[1m\u001b[91m$\u001b[0m, aligment score: 0.7839125394821167\n","\u001b[1m\u001b[94mdollars\u001b[0m:\n","you would n't shit on me for a billion \u001b[1m\u001b[94mdollars\u001b[0m ?\n","\n","\u001b[1m\u001b[91m$\u001b[0m:\n","i would pay \u001b[1m\u001b[91m$\u001b[0m 300 . i love having smooth pits and it 's just for me .\n","\n","~~~\n","\u001b[1m\u001b[94mweights\u001b[0m & \u001b[1m\u001b[91mexercise\u001b[0m, aligment score: 0.7828139066696167\n","\u001b[1m\u001b[94mweights\u001b[0m:\n","naturally : get a blood test to see if you have any deficiencies . vitamin d , magnesium are common . if you have a deficiency , try to fix your diet or use supplements . eat a healthy , diverse diet . do some exercise , ideally cardio and \u001b[1m\u001b[94mweights\u001b[0m . non naturally : use exogenous testosterone , either injections , gels or maybe ( but not recommend ) orally . do not do this without your doctor . ask about trt and he 'll tell you if you need it .\n","\n","\u001b[1m\u001b[91mexercise\u001b[0m:\n","if i could give myself any advice from back when i was in my mid 20 ' s it would be to get comfortable with the unknown . being lost is definitely something that comes and goes . in my 20 ' s i 'd wake up thinking i got a grip on life then the next day i did n't know shit about nothin . the quicker you accept that you are ever changing and evolving the easier it is to \u001b[1m\u001b[91mexercise\u001b[0m your problem solving skills .\n","\n","~~~\n","\u001b[1m\u001b[94mbruh\u001b[0m & \u001b[1m\u001b[91mlol\u001b[0m, aligment score: 0.7804096341133118\n","\u001b[1m\u001b[94mbruh\u001b[0m:\n","you ever hugged on mdma ? life changing , \u001b[1m\u001b[94mbruh\u001b[0m\n","\n","\u001b[1m\u001b[91mlol\u001b[0m:\n","a playful tickle is nice every now and then . running fingers across my legs tickles but it gives me good chills . actual tickling would kill the mood a bit , but i 'd mostly just be confused . \u001b[1m\u001b[91mlol\u001b[0m\n","\n","~~~\n","\u001b[1m\u001b[94msuit\u001b[0m & \u001b[1m\u001b[91mdress\u001b[0m, aligment score: 0.7799713015556335\n","\u001b[1m\u001b[94msuit\u001b[0m:\n","i used to wear a \u001b[1m\u001b[94msuit\u001b[0m to work every day and would get the occasional funny look in the office . however , i received a lot of compliments . some people liked to turn up in scabby shoes and a shirt , yet every day without fail i turned up in a \u001b[1m\u001b[94msuit\u001b[0m . i had the mindset of dress for the job you want , not the job you have . i used it to boost my confidence . 1 - dress well and flaunt your own style . 2 - people will notice you for the effort you make . 3 - you deserve to dress well just as much as the next guy . 4 - keep dressing well and you 'll learn to dismiss the insecure thoughts you have .\n","\n","\u001b[1m\u001b[91mdress\u001b[0m:\n","oooh yes i had a dark blue formal \u001b[1m\u001b[91mdress\u001b[0m ( not a real wedding \u001b[1m\u001b[91mdress\u001b[0m ) and even thought everyone was against it , i still wore it . made me feel much better and confident than the white fluffy \u001b[1m\u001b[91mdress\u001b[0mes that made me look like a freaking fat marshmallow :see-no-evil_monkey: :rolling_on_the_floor_laughing:\n","\n","~~~\n","\u001b[1m\u001b[94mfilter\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7793443202972412\n","\u001b[1m\u001b[94mfilter\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have re-read your question and still think this is a failure of the automated \u001b[1m\u001b[94mfilter\u001b[0m , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","your submission has been removed because this question is a commonly asked and we are not accepting any new submissions at this time . please view our faq [ here ] ( HTTPURL ) or search the subreddit . you can always go to r / askwomenadvice if you are seeking advice on a personal situation or r / findareddit to find the right place for your post . if you think you received this message in error , link your post & please [ message the moderators ( click here ) ] ( HTTPURL post was auto-removed for asking for asking about a commonly asked subject . \" & message =) . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mre-read\u001b[0m & \u001b[1m\u001b[91mcompose\u001b[0m, aligment score: 0.7788638472557068\n","\u001b[1m\u001b[94mre-read\u001b[0m:\n","your submission was removed by a computer . this could be for a number of reasons , most of which are summarized in the rules text on the right . in most of these cases , the computer is right , and we will not overturn its decision . if you have \u001b[1m\u001b[94mre-read\u001b[0m your question and still think this is a failure of the automated filter , message us with an actual reason as to why the computer is wrong . if you just say that you think the computer is wrong without any reasoning , we will ignore you . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / compose / ? to =/ r / askmen ) if you have any questions or concerns . *\n","\n","\u001b[1m\u001b[91mcompose\u001b[0m:\n","hello randomstufflol 1234 . thank you for participating in / r / askwomen . however , your submission has been removed , because we do not allow personal advice or evaluation submissions . you can always go to / r / askwomenadvice , / r / relationships for romantic / non-romantic relationship advice , / r / legaladvice for legal advice , / r / femalefashionadvice for fashion advice , / r / skincareaddiction for skin care advice , or / r / findareddit if you dont know where else to go . please remember to read the rules of any subreddit you go to . if you think you received this message in error , please [ message the moderators ( click here ) ] ( HTTPURL post was removed for personal advice \" & message =) . do n't forget to link your post ! thanks . * i am a bot , and this action was performed automatically . please [ contact the moderators of this subreddit ] ( / message / \u001b[1m\u001b[91mcompose\u001b[0m / ? to =/ r / askwomen ) if you have any questions or concerns . *\n","\n","~~~\n","\u001b[1m\u001b[94mtrade\u001b[0m & \u001b[1m\u001b[91meducation\u001b[0m, aligment score: 0.772612988948822\n","\u001b[1m\u001b[94mtrade\u001b[0m:\n","getting into a \u001b[1m\u001b[94mtrade\u001b[0m ( welding ) . i now have a job with vision , dental and health insurance . matching 401k , retirement fund , pto , paid holidays , reimbursement for any gear for work , raises given out “ whenever someone feels you 've improved ” ( its actually an amazing system , if you dont have a raise or three in 90 days your doing something wrong ) , oh and my contract states my mmj card wont be a hindrance or reason to deny workmen 's compensation / exempts me from drug testing .\n","\n","\u001b[1m\u001b[91meducation\u001b[0m:\n","paying for \u001b[1m\u001b[91meducation\u001b[0m .\n","\n","~~~\n","\u001b[1m\u001b[94mcash\u001b[0m & \u001b[1m\u001b[91mdebt\u001b[0m, aligment score: 0.7705771923065186\n","\u001b[1m\u001b[94mcash\u001b[0m:\n","damn . this is my story too . my company paid in \u001b[1m\u001b[94mcash\u001b[0m . no taxes . but i was an idiot college kid . did n't know it was illegal and the consequences . they went bankrupt a year later . i realized too late .\n","\n","\u001b[1m\u001b[91mdebt\u001b[0m:\n","pay off more \u001b[1m\u001b[91mdebt\u001b[0m . mortgage , car loans , credit cards . you have a lot more freedom when the bank does n't own your ass .\n","\n","~~~\n","\u001b[1m\u001b[94mknees\u001b[0m & \u001b[1m\u001b[91marm\u001b[0m, aligment score: 0.7678806185722351\n","\u001b[1m\u001b[94mknees\u001b[0m:\n","i 'm uncut and damn my dick gets beat up if i have sex multiple times in a night ! ouch . rubbed my \u001b[1m\u001b[94mknees\u001b[0m raw having sex on a couch once . ouch . bad hand jobs can hurt . i had to stop a girl once from yanking my foreskin up and down so hard .\n","\n","\u001b[1m\u001b[91marm\u001b[0m:\n","helped a friend move into her first place back at the start of college , she was moving out of her parents house and had no car and no one to help her i used my brother 's car and drove her back and forth between her parent 's house and her new place which was very far away from 8 am to 11 pm at night we drove back and forth moving boxes , i paid for the gas , the lunch , everything i never got a single thank you from her and all she did was complain the entire time about how hot it was and to top it off she hosted a house w\u001b[1m\u001b[91marm\u001b[0ming party and did n't invite me because she \" forgot \" i now do n't bust my ass helping people as much as i used to .\n","\n","~~~\n","\u001b[1m\u001b[94mpiss\u001b[0m & \u001b[1m\u001b[91mpee\u001b[0m, aligment score: 0.7659830451011658\n","\u001b[1m\u001b[94mpiss\u001b[0m:\n","well if he is nonverbal then you must teach him through non-verbal methods , i am assuming he has some kind of retardation ? basically , he is more animalistic than us and therefore should be educated through his amygdala , which is fear / excitement-based learning . he should be beaten into submission everytime he misbehaves until it clicks that violence = i am hurt . sorry not sorry we are all fucking animals at the end and some are n't capable of learning through conventional methods . this case \u001b[1m\u001b[94mpiss\u001b[0mes me off a lot more than it should i must say , prolly because something similar happened to me not so long ago lmao\n","\n","\u001b[1m\u001b[91mpee\u001b[0m:\n","driving . people driving slowly in the left lane . people driving below the s\u001b[1m\u001b[91mpee\u001b[0md limit during traffic . people not using their indicators . i cuss like a sailor m-f . it 's the only time i hate people .\n","\n","~~~\n"]}],"source":["# MY DATASET\n","\n","for pair, score in sorted(zip(translations, scores), key=lambda x:x[1], reverse=True)[:20]:\n","    word_a_highlighted = '{}{}{}'.format(color.BOLD + color.BLUE, pair[0], color.END)\n","    word_b_highlighted = '{}{}{}'.format(color.BOLD + color.RED, pair[1], color.END)\n","    print(word_a_highlighted + ' & ' + word_b_highlighted + ', aligment score: {}'.format(score))\n","    \n","    # print word_a_highlighted, and example sentences\n","    print('{}:'.format(word_a_highlighted))\n","    [print(s.replace(pair[0], word_a_highlighted)) for s in get_example_sentences(pair[0], corpus_a, n_sentences=n_sentences, seed=seed)]\n","    \n","    # print word_b_highlighted, and example sentences\n","    print('{}:'.format(word_b_highlighted))\n","    [print(s.replace(pair[1], word_b_highlighted)) for s in get_example_sentences(pair[1], corpus_b, n_sentences=n_sentences, seed=seed)]\n","    \n","    print('~~~')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2B5rB1AdMi-"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"Analysis.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}